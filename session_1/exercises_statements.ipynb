{"cells":[{"cell_type":"markdown","metadata":{"id":"eXl5PzoKDEmK"},"source":["#Lecture 1 - AE, VAE, Latent space"]},{"cell_type":"markdown","source":["# Theoretical Questions"],"metadata":{"id":"bURoJfYdP-R6"}},{"cell_type":"markdown","source":["### TQ1. How could you implement an AutoEncoder (or VAE) for Images? And time series?\n"],"metadata":{"id":"AWbUX_t-QHtn"}},{"cell_type":"markdown","source":["Response -\n"],"metadata":{"id":"I2pCzZxDQUKu"}},{"cell_type":"markdown","source":["### TQ2. Aside from the limitations listed in class, what are others limitations to AutoEncoders?"],"metadata":{"id":"vPxVfGpFQHXC"}},{"cell_type":"markdown","source":["Response -\n"],"metadata":{"id":"ESul0BdkQSl9"}},{"cell_type":"markdown","source":["### TQ3. What effect does the bottleneck have in the network?"],"metadata":{"id":"OZgnr3agQHd4"}},{"cell_type":"markdown","source":["Response -\n"],"metadata":{"id":"utFW8JuJQTJY"}},{"cell_type":"markdown","source":["### TQ4. Imagine we want to build an AutoEncoder to fix missing data. Assume we have a reasonably sized dataset with enough non-faulty data to train this AutoEncoder and a massive dataset to fix."],"metadata":{"id":"46Ie0gSgQHmU"}},{"cell_type":"markdown","source":["Response -\n"],"metadata":{"id":"iZj4UftPQTn1"}},{"cell_type":"markdown","source":["# Code Exercises"],"metadata":{"id":"8mD5xUjD-Bg0"}},{"cell_type":"markdown","metadata":{"id":"w3uHCp0VDEmM"},"source":["**Dependencies**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SArzmQSlDEmO"},"outputs":[],"source":["!pip install pytorch-lightning"]},{"cell_type":"markdown","metadata":{"id":"O23KCJuqDEmQ"},"source":["Load your drive using this (if necessary):"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OCtv-uFjDEmQ"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["Set up dependencies of the notebook"],"metadata":{"id":"LoooK3X0ajZ1"}},{"cell_type":"code","source":["import datetime\n","\n","import torch\n","import torch.nn as nn\n","\n","import pytorch_lightning as pl\n","import torchmetrics\n","from pytorch_lightning import seed_everything\n","\n","import numpy as np\n","\n","import pandas as pd\n","\n","from sklearn.preprocessing import OneHotEncoder\n","\n","import matplotlib.pyplot as plt"],"metadata":{"id":"hawWNquoaivm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Data Loading\n","Data loading is done here to simplify the coding exercises. The Data Loading and most of the training will remain the same. However, some changes will be required."],"metadata":{"id":"nl2-gyQIHVRY"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"C73taijTDEmT"},"outputs":[],"source":["DATA_PATH = 'mushrooms.csv'\n","SEED = 42\n","seed_everything(seed=SEED) # Set seed for reproducibility,"]},{"cell_type":"markdown","metadata":{"id":"WZ2nhhgyDEmU"},"source":["Load the dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ew5mcGB2DEmV"},"outputs":[],"source":["mush_df = pd.read_csv(DATA_PATH) # Drop the class,\n","mush_df.head()"]},{"cell_type":"markdown","source":["Now, quicly perform the One Hot Encoding with sklearn `sklearn.preprocessing.OneHotEncoder`\n","\n"],"metadata":{"id":"n1x0Y7D2jH1c"}},{"cell_type":"code","source":["ohe = OneHotEncoder(sparse_output=False)\n","\n","mush_df_train = mush_df.drop('class', axis=1).copy()\n","new_data = ohe.fit_transform(mush_df_train)\n","mush_df_train = pd.DataFrame(new_data, columns=ohe.get_feature_names_out())\n","mush_df_train\n"],"metadata":{"id":"kaKnuAM3iRwY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Torch dataset"],"metadata":{"id":"W9ReGHqHjvh1"}},{"cell_type":"code","source":["class MushroomDataset(torch.utils.data.Dataset):\n","    def __init__(self, df):\n","        self.df = df\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, idx):\n","        return torch.tensor(self.df.iloc[idx].values, dtype=torch.float32)"],"metadata":{"id":"JK1VLcw0j0Od"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dqcWKZuNDEmX"},"source":["Finally, declare the `pl.LightningDataModule`"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3fE7hB7SDEmX"},"outputs":[],"source":["class MushroomModule(pl.LightningDataModule):\n","    def __init__(self, df, batch_size=16, val_size=0.2):\n","        super().__init__()\n","        self.batch_size = batch_size\n","        self.val_size = val_size\n","        self.full_len = len(df)\n","        self.df = df\n","\n","    def setup(self, stage=None):\n","        # We can't perform numerical evaluations! Only train + val\n","        train_end = int((1 - self.val_size) * self.full_len)\n","        self.train_data = MushroomDataset(self.df[:train_end])\n","        self.val_data = MushroomDataset(self.df[train_end:])\n","\n","    def collate_fn(self, batch):\n","        features = torch.stack(batch, axis=0)  # [batch_size, input_size]\n","        return features\n","\n","    def train_dataloader(self):\n","        return torch.utils.data.DataLoader(self.train_data, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n","\n","    def val_dataloader(self):\n","        return torch.utils.data.DataLoader(self.val_data, batch_size=self.batch_size, shuffle=False, collate_fn=self.collate_fn)"]},{"cell_type":"markdown","metadata":{"id":"RLPAFe08DEmR"},"source":["## CQ1 (✰). Improving the autoencoder\n","\n","In the Lecture we made a basic AutoEncoder, however, this autoencoder is severely flawed and can be improved upon with better design decisions along the Encoder-Decoder architecture.\n","\n","1.   Labels are one hot encodings, ranging from 0 to 1; if the network outputs conform to these ranges, it can be expected to perform better. Specially with MSE loss. There's a catch to this, though...\n","2.   ... Several positions of the output vector match with a single feature. Measuring MSE is ill-fitted, as there are several classification tasks we could use a loss term for each classification task at the same time.\n","\n","Intorduce any desired changes to the network. Generalize the number of layers it may have, include regularization... include your own improvements.\n","\n"]},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_size, hidden_dim, bottleneck):\n","        super(Encoder, self).__init__()\n","\n","    def forward(self, x):\n","        pass"],"metadata":{"id":"pvwf4dwjOc23"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, bottleneck, hidden_dim, output_size):\n","        super(Decoder, self).__init__()\n","\n","    def forward(self, x):\n","        pass"],"metadata":{"id":"vc0aaQi-mtE3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class AutoEncoder(nn.Module):\n","    def __init__(self, input_size, hidden_dim, bottleneck):\n","        super(AutoEncoder, self).__init__()\n","\n","    def forward(self, x):\n","        pass"],"metadata":{"id":"UURO7AqYmz0_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training\n","\n","Introduce any new change you want in the `compute_batch` method."],"metadata":{"id":"nw9029HfHvo0"}},{"cell_type":"code","source":["class MushroomCompressor(pl.LightningModule):\n","    def __init__(self, model, categories, learning_rate=1e-3, weight_decay=0.):\n","        super(MushroomCompressor,self).__init__()\n","        self.save_hyperparameters() # Save Hyperparams\n","        self.learning_rate = learning_rate\n","        self.weight_decay = weight_decay\n","        self.model = model\n","        self.criterion = nn.CrossEntropyLoss()\n","        self.nrmse = torchmetrics.NormalizedRootMeanSquaredError()\n","        self.mse = nn.MSELoss()\n","        self.categories = categories # New property, required for advanced train\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def compute_batch(self, batch, split='train'):\n","        pass\n","\n","    def training_step(self, batch, batch_idx):\n","        return self.compute_batch(batch, 'train')\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self.compute_batch(batch, 'val')\n","\n","    def predict_step(self, batch, batch_idx):\n","        return self(batch)[-1]\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate,\n","                                 weight_decay=self.weight_decay) # self.parameters() son los parámetros del modelo"],"metadata":{"id":"dDTLcE8pgR50"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Hyper-parameters\n","\n","LEARNING_RATE = 1e-3 #@param {type:\"number\"}\n","WEIGHT_DECAY = 0. #@param {type:\"number\"}\n","BATCH_SIZE = 8 # @param [\"2\",\"4\",\"8\",\"16\",\"32\"] {\"type\":\"raw\"}\n","MAX_EPOCHS = 10 # @param {\"type\":\"slider\",\"min\":0,\"max\":100,\"step\":1}\n","HIDDEN_DIM = 32 # @param {\"type\":\"slider\",\"min\":0,\"max\":128,\"step\":1}\n","LATENT_DIM = 3 # @param {\"type\":\"slider\",\"min\":0,\"max\":16,\"step\":1}\n","\n","SAVE_DIR = f'lightning_logs/sales/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n"],"metadata":{"id":"_AGFcyMNoVw6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Train loop"],"metadata":{"id":"jp4kq1lmidvQ"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gy4p_UO3DEmb"},"outputs":[],"source":["# DataModule\n","data_module = MushroomModule(mush_df_train, batch_size=BATCH_SIZE)\n","\n","# Model\n","model = AutoEncoder(input_size=mush_df_train.values.shape[1], hidden_dim=HIDDEN_DIM, bottleneck=LATENT_DIM)\n","\n","# LightningModule\n","module = MushroomCompressor(model, ohe.categories_, learning_rate=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n","\n","# Callbacks\n","early_stopping_callback = pl.callbacks.EarlyStopping(\n","    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n","    mode='min',\n","    patience=5, # número de epochs sin mejora antes de parar\n","    verbose=False, # si queremos que muestre mensajes del estado del early stopping\n",")\n","model_checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n","    mode='min', # queremos minimizar la pérdida\n","    save_top_k=1, # guardamos solo el mejor modelo\n","    dirpath=SAVE_DIR, # directorio donde se guardan los modelos\n","    filename=f'best_model' # nombre del archivo\n",")\n","\n","callbacks = [early_stopping_callback, model_checkpoint_callback]\n","\n","# Loggers\n","csv_logger = pl.loggers.CSVLogger(\n","    save_dir=SAVE_DIR,\n","    name='metrics',\n","    version=None\n",")\n","\n","loggers = [csv_logger] # se pueden poner varios loggers (mirar documentación)\n","\n","# Trainer\n","trainer = pl.Trainer(max_epochs=MAX_EPOCHS, accelerator='gpu', callbacks=callbacks,\n","                     logger=loggers, precision='bf16')\n","\n","trainer.fit(module, data_module)"]},{"cell_type":"markdown","metadata":{"id":"mvMyoxtjDEmb"},"source":["### Visualization\n","Lets explore the latent space!"]},{"cell_type":"code","source":["embeddings = trainer.predict(module, torch.utils.data.DataLoader(MushroomDataset(mush_df_train), batch_size=16))\n","embeddings = torch.cat(embeddings).to(torch.float).cpu().numpy()\n","embeddings"],"metadata":{"id":"Mtv1FxmPu30l"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","# You can visualize a 3d space with this!\n","fig = plt.figure(figsize=(10, 8))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","classes = mush_df['class']\n","\n","scatter = ax.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2], c=classes.astype('category').cat.codes)\n","\n","ax.set_xlabel(\"Latent Dimension 1\")\n","ax.set_ylabel(\"Latent Dimension 2\")\n","ax.set_zlabel(\"Latent Dimension 3\")\n","ax.set_title(\"Latent Space Representation of Mushrooms\")\n","\n","handles, labels = scatter.legend_elements()\n","ax.legend(handles, classes.unique(), title=\"Classes\")\n","\n","plt.show()"],"metadata":{"id":"7gZ8qkzgiP0f"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## CQ2 (✰✰). Implement the VAE for the mushroom dataset\n","\n","We have seen that VAEs overcome the problems of AE by generating a continuous latent space. We implement here this VAE model. The VAE generates the average and deviation of a distribution, instead of a point. We need to implement an encoder that performs exactly that task.\n","\n","We have to build $σ(x)$ and $μ(x)$, these transformations operate on $x$ (the original latent representation), the earlier point that was generated by the encoder. Use a Linear layer to let the model decide where's the average of $x$ and another Linear layer to let the model decide what's the deviation of $x$.\n","\n","**Note**: For better results we use the logarithmic deviation. This, however, does not affect the encoder.\n","\n"],"metadata":{"id":"091uAo7I-rcz"}},{"cell_type":"markdown","source":["### VAE definition"],"metadata":{"id":"3I8mR4OJETd4"}},{"cell_type":"code","source":["class Encoder(nn.Module):\n","    def __init__(self, input_size, hidden_dim, bottleneck):\n","        super(Encoder, self).__init__()\n","        self.encoder_layer = nn.Sequential(\n","            nn.Linear(input_size, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, bottleneck),\n","            nn.BatchNorm1d(bottleneck),\n","        )\n","        self.mu = nn.Linear(bottleneck, bottleneck)\n","        self.sigma = nn.Linear(bottleneck, bottleneck)\n","\n","    def forward(self, x):\n","        pass"],"metadata":{"id":"CL4camo0_7AS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now, the decoder will continue being the same, it receives a latent representation and generates a mushroom. Same as before."],"metadata":{"id":"mTsbg6KYBDSL"}},{"cell_type":"code","source":["class Decoder(nn.Module):\n","    def __init__(self, bottleneck, hidden_dim, output_size):\n","        super(Decoder, self).__init__()\n","        self.decoder_layer = nn.Sequential(\n","            nn.Linear(bottleneck, hidden_dim),\n","            nn.GELU(),\n","            nn.Linear(hidden_dim, output_size),\n","        )\n","\n","    def forward(self, x):\n","        pass"],"metadata":{"id":"-dryz4m0BLYt"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["However, there is a mismatch now. The encoder outputs a distribution of points, while the decoder takes a single point. Thus, the Encoder-Decoder should sample the distribution of the encoder using $μ(x)$ and $σ(x)$!\n","\n","However, sampling is non-differentiable, we need the **reparametrization trick**. Normally we would use `torch.normal(mean=μ(x), std=σ(x))`, however we will use a different expression.\n","\n","The reparamerization trick involves using `torch.randn(bottleneck)` to obtain a random vector to operate with the average and deviation.\n","\n","$$x_{sampled} = μ(x) + N(0,1) \\cdot e^{0.5*σ(x)}  $$\n","\n","Or, the exact same would be multiplying the deviation ($\\sigma(x)$) by a standard normal random distribution ($N(0,1)$) then summing the mean $\\mu(x)$.\n","\n","To train we also need to retrieve the deviation and mean!"],"metadata":{"id":"RNSnP2U8BTKW"}},{"cell_type":"code","source":["class VAE(nn.Module):\n","    def __init__(self, input_size, hidden_dim, bottleneck):\n","        super(VAE, self).__init__()\n","        self.encoder = Encoder(input_size, hidden_dim, bottleneck)\n","        self.decoder = Decoder(bottleneck, hidden_dim, input_size)\n","\n","    def forward(self, x):\n","        pass"],"metadata":{"id":"f9br1veCEAdm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Training\n","\n","The new loss term is different from before. We use:\n","\n","$$ℒ(x',x) = ℒ_{rec}(x', x) + λℒ_{kl}(\\mu(x), \\sigma(x))$$\n","\n","This is a sum of the reconstruction loss (Whatever loss we choose, MSE, CE, etc) and the Kullback–Leibler loss (KL Div.). $x$ are the inputs, $x'$ is the reconstruction.\n","\n","Modify `_rec_loss(self, preds, targets)` and  `_kl_loss(self, mu, sigma)` to build the new `compute_batch(self, batch)`.\n","\n","You can use either the torch implementation of the [KL-Divergence](https://pytorch.org/docs/stable/generated/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss) or code it yourself!"],"metadata":{"id":"YSuGRfdCEO_4"}},{"cell_type":"code","source":["class MushroomGenerator(pl.LightningModule):\n","    def __init__(self, model, categories, learning_rate=1e-3, weight_decay=0., lambd=1.):\n","        super(MushroomGenerator,self).__init__()\n","        self.save_hyperparameters() # Save Hyperparams\n","        self.learning_rate = learning_rate\n","        self.weight_decay = weight_decay\n","        self.model = model\n","        self.reconstruction = nn.CrossEntropyLoss()\n","        self.nrmse = torchmetrics.NormalizedRootMeanSquaredError()\n","        self.mse = nn.MSELoss()\n","        self.lambd = lambd # Weights the influence of the KLDiv\n","        self.categories = categories\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","    def _rec_loss(self, preds, targets):\n","        pass\n","\n","    def _kl_loss(self, mu, sigma):\n","        pass\n","\n","    def compute_batch(self, batch, split='train'):\n","        pass\n","\n","    def training_step(self, batch, batch_idx):\n","        return self.compute_batch(batch, 'train')\n","\n","    def validation_step(self, batch, batch_idx):\n","        return self.compute_batch(batch, 'val')\n","\n","    def predict_step(self, batch, batch_idx):\n","        return self(batch)[-1][0]\n","\n","    def configure_optimizers(self):\n","        return torch.optim.AdamW(self.parameters(), lr=self.learning_rate,\n","                                 weight_decay=self.weight_decay)"],"metadata":{"id":"mUyObKv3ENax"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#@title Hyper-parameters\n","\n","LEARNING_RATE = 1e-3 #@param {type:\"number\"}\n","WEIGHT_DECAY = 0. #@param {type:\"number\"}\n","BATCH_SIZE = 8 # @param [\"2\",\"4\",\"8\",\"16\",\"32\"] {\"type\":\"raw\"}\n","MAX_EPOCHS = 10 # @param {\"type\":\"slider\",\"min\":0,\"max\":100,\"step\":1}\n","HIDDEN_DIM = 32 # @param {\"type\":\"slider\",\"min\":0,\"max\":128,\"step\":1}\n","LATENT_DIM = 3 # @param {\"type\":\"slider\",\"min\":0,\"max\":16,\"step\":1}\n","LAMBDA = 1 #@param {type:\"number\"}\n","\n","SAVE_DIR = f'lightning_logs/sales/{datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")}'\n"],"metadata":{"id":"qkdU4KVqINtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Run!"],"metadata":{"id":"PzoFj6MdIPm-"}},{"cell_type":"code","source":["# DataModule\n","data_module = MushroomModule(mush_df_train, batch_size=BATCH_SIZE)\n","\n","# Model\n","model = VAE(input_size=mush_df_train.values.shape[1], hidden_dim=HIDDEN_DIM, bottleneck=LATENT_DIM)\n","\n","# LightningModule\n","module = MushroomGenerator(model, ohe.categories_,\n","                           learning_rate=LEARNING_RATE,\n","                           weight_decay=WEIGHT_DECAY,\n","                           lambd=LAMBDA)\n","\n","# Callbacks\n","early_stopping_callback = pl.callbacks.EarlyStopping(\n","    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n","    mode='min',\n","    patience=5, # número de epochs sin mejora antes de parar\n","    verbose=False, # si queremos que muestre mensajes del estado del early stopping\n",")\n","model_checkpoint_callback = pl.callbacks.ModelCheckpoint(\n","    monitor='val_loss', # monitorizamos la pérdida en el conjunto de validación\n","    mode='min', # queremos minimizar la pérdida\n","    save_top_k=1, # guardamos solo el mejor modelo\n","    dirpath=SAVE_DIR, # directorio donde se guardan los modelos\n","    filename=f'best_model' # nombre del archivo\n",")\n","\n","callbacks = [early_stopping_callback, model_checkpoint_callback]\n","\n","# Loggers\n","csv_logger = pl.loggers.CSVLogger(\n","    save_dir=SAVE_DIR,\n","    name='metrics',\n","    version=None\n",")\n","\n","loggers = [csv_logger] # se pueden poner varios loggers (mirar documentación)\n","\n","# Trainer\n","trainer = pl.Trainer(max_epochs=MAX_EPOCHS, accelerator='gpu', callbacks=callbacks,\n","                     logger=loggers, precision='bf16')\n","\n","trainer.fit(module, data_module)"],"metadata":{"id":"-5hcfh4CIKN2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embeddings = trainer.predict(module, torch.utils.data.DataLoader(MushroomDataset(mush_df_train), batch_size=16))\n","embeddings = torch.cat(embeddings).to(torch.float).cpu().numpy()\n","embeddings"],"metadata":{"id":"C8GeJ8n1Lh_W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","from mpl_toolkits.mplot3d import Axes3D\n","\n","# You can visualize a 3d space with this!\n","fig = plt.figure(figsize=(10, 8))\n","ax = fig.add_subplot(111, projection='3d')\n","\n","classes = mush_df['class']\n","\n","scatter = ax.scatter(embeddings[:, 0], embeddings[:, 1], embeddings[:, 2], c=classes.astype('category').cat.codes)\n","\n","ax.set_xlabel(\"Latent Dimension 1\")\n","ax.set_ylabel(\"Latent Dimension 2\")\n","ax.set_zlabel(\"Latent Dimension 3\")\n","ax.set_title(\"Latent Space Representation of Mushrooms\")\n","\n","handles, labels = scatter.legend_elements()\n","ax.legend(handles, classes.unique(), title=\"Classes\")\n","\n","plt.show()"],"metadata":{"id":"ILe8s4IyLvDP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["At this point you won't notice any difference between the AE and the VAE... and that's okay. It's difficult to understand the point of the VAE with categorical data, it will be apparent in the next Lecture with images."],"metadata":{"id":"xUCmRXkwNuHT"}}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}