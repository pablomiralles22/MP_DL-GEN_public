{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 5 - Exercise notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CQ1. (☆☆) Complete the following Byte-Pair Encoding tokenizer with the algorithm from the slides"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BPEBasicTokenizer:\n",
    "    \"\"\"\n",
    "    A simple Byte Pair Encoding tokenizer that learns a vocabulary from a given text\n",
    "    and can encode/decode text into/from a list of tokens\n",
    "\n",
    "    Based on: https://github.com/karpathy/minbpe/blob/master/minbpe/base.py, with LICENSE:\n",
    "\n",
    "    The MIT License (MIT) Copyright (c) 2020 Andrej Karpathy\n",
    "\n",
    "    Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n",
    "\n",
    "    The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n",
    "\n",
    "    THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Initialize class variables\n",
    "        self.merges = None\n",
    "        self.vocab = None\n",
    "\n",
    "    def train(self, text, vocab_size, verbose=False):\n",
    "        \"\"\"\n",
    "        Given a text and a desired vocabulary size, learn a vocabulary\n",
    "        of vocab_size tokens using Byte Pair Encoding (BPE)\n",
    "        \"\"\"\n",
    "        assert vocab_size >= 256  # 256 is the number of possible bytes, which are the initial vocabulary\n",
    "        num_merges = vocab_size - 256  # number of merges to perform to reach the desired vocab size\n",
    "\n",
    "        # Step 1. Preprocess text, converting it to bytes and then to a token list\n",
    "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "        idxs = list(text_bytes) # list of integers in range 0..255\n",
    "\n",
    "        # Step 2. Iteratively merge the most common pairs to create new tokens\n",
    "        merges = {}  # (int, int) -> int\n",
    "        vocab = {idx: bytes([idx]) for idx in range(256)}  # int -> bytes\n",
    "\n",
    "        for i in range(num_merges):\n",
    "            if len(idxs) < 2:\n",
    "                print(f\"WARNING: less than 2 tokens left, stopping early at merge {i}\")\n",
    "                break\n",
    "\n",
    "            # Step 2.1. Count up the number of times every consecutive pair appears\n",
    "            pair_counts = self._get_pair_counts(idxs)\n",
    "\n",
    "            # Step 2.2. Find the pair with the highest count\n",
    "            best_pair =  # TODO: find the pair with the highest count\n",
    "\n",
    "            # Step 2.3. Create a new token: assign it the next available id\n",
    "            new_idx = 256 + i\n",
    "\n",
    "            # Step 2.4. Replace all occurrences of best_pair in idxs with new_idx\n",
    "            idxs = self._merge_pair(idxs, best_pair, new_idx)\n",
    "\n",
    "            # Step 2.5. Save the merge\n",
    "            merges[best_pair] =  # TODO: save the new index for the pair\n",
    "            vocab[new_idx] =  # TODO: save the new token, which is the concatenation of the two merged tokens\n",
    "\n",
    "            # Print progress if verbose is True\n",
    "            if verbose is True:\n",
    "                print(f\"merge {i+1}/{num_merges}: {best_pair} -> {new_idx} ({vocab[new_idx]}) had {pair_counts[best_pair]} occurrences\")\n",
    "\n",
    "        # Step 3. Save class variables\n",
    "        self.merges = merges # used in encode()\n",
    "        self.vocab = vocab   # used in decode()\n",
    "\n",
    "    def encode(self, text: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Given a Python string, return a list of token indices (integers)\n",
    "        \"\"\"\n",
    "        # Step 1. Convert the Python string to a list of bytes\n",
    "        text_bytes = text.encode(\"utf-8\") # raw bytes\n",
    "\n",
    "        # Step 2. Convert the list of bytes to a list of integers\n",
    "        idxs = list(text_bytes) # list of integers in range 0..255\n",
    "\n",
    "        # Step 3. Iteratively merge pairs until no more merges are possible\n",
    "        while len(idxs) >= 2:\n",
    "            # Step 3.1. Count up the number of times every consecutive pair appears in idxs\n",
    "            pair_counts = self._get_pair_counts(idxs)\n",
    "\n",
    "            # Step 3.2. Check that any of the pairs can be merged\n",
    "            any_merge_possible = any(pair in self.merges for pair in pair_counts)\n",
    "            if any_merge_possible is False:\n",
    "                break\n",
    "\n",
    "            # Step 3.3. Find the pair with the lowest merge index\n",
    "            best_pair =  # TODO: find the pair with the lowest index (i.e., the one that was merged first)\n",
    "\n",
    "            # Step 3.4. Merge the best pair\n",
    "            merge_idx = self.merges[best_pair]\n",
    "            idxs = self._merge_pair(idxs, best_pair, merge_idx)\n",
    "\n",
    "        return idxs\n",
    "\n",
    "    def decode(self, idxs: list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Given token indices (list of integers), return the decoded Python string\n",
    "        \"\"\"\n",
    "        # Step 1. Convert the list of integers to a list of bytes using the vocab dictionary\n",
    "        byte_list = [self.vocab[idx] for idx in idxs]\n",
    "        # Step 2. Join the list of bytes into a single byte string\n",
    "        text_bytes = b\"\".join(byte_list)\n",
    "        # Step 3. Decode the byte string into a Python string\n",
    "        return text_bytes.decode(\"utf-8\", errors=\"replace\")\n",
    "\n",
    "    def _get_pair_counts(self, idxs: list[int]) -> dict[tuple[int, int], int]:\n",
    "        \"\"\"\n",
    "        Given a list of integers (idxs), return a dict or Counter object with the\n",
    "        counts of all consecutive pairs of integers\n",
    "        Example: idxs=[1, 2, 3, 1, 2] -> dict({(1, 2): 2, (2, 3): 1, (3, 1): 1})\n",
    "        \"\"\"\n",
    "        # TODO: implement this function\n",
    "\n",
    "    def _merge_pair(self, idxs, pair, new_idx):\n",
    "        \"\"\"\n",
    "        In the list of integers (ids), replace all consecutive occurrences\n",
    "        of pair with the new integer token new_idx\n",
    "        Example: ids=[1, 2, 3, 1, 2], pair=(1, 2), idx=4 -> [4, 3, 4]\n",
    "        \"\"\"\n",
    "        new_idxs = []\n",
    "        j = 0\n",
    "        while j + 1 < len(idxs):\n",
    "            if idxs[j] == pair[0] and idxs[j+1] == pair[1]:\n",
    "                # TODO: replace the pair with the new_idx\n",
    "            else:\n",
    "                # TODO: keep the current integer\n",
    "\n",
    "        if j < len(idxs): new_idxs.append(idxs[j])\n",
    "\n",
    "        return new_idxs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I include some test cases now so you can check for any mistakes before looking at the solution. We begin by download the shakespeare data to train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "DATA_DIR = \"data\"\n",
    "DATA_PATH = f\"{DATA_DIR}/shakespeare.txt\"\n",
    "\n",
    "def load_shakespeare():\n",
    "    # Download Shakespeare's works\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        response = requests.get(DATA_URL)\n",
    "        with open(DATA_PATH, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Load the text\n",
    "    with open(DATA_PATH, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Test case 1: Basic training and encoding\n",
    "def test_gets_desired_vocab_size():\n",
    "    text = load_shakespeare()[:10000]  # use only the first 10k characters\n",
    "    tokenizer = BPEBasicTokenizer()\n",
    "    tokenizer.train(text, vocab_size=300, verbose=False)\n",
    "    assert len(tokenizer.vocab) == 300, f\"Expected vocab size 300, got {len(tokenizer.vocab)}\"\n",
    "\n",
    "# Test case 2: Encoding and decoding\n",
    "def test_encodes_and_decodes():\n",
    "    train_text = load_shakespeare()[:10000]  # use only the first 10k characters\n",
    "    tokenizer = BPEBasicTokenizer()\n",
    "    tokenizer.train(train_text, vocab_size=300, verbose=False)\n",
    "\n",
    "    test_samples = [\n",
    "        \"To be or not to be, that is the question.\",\n",
    "        \"All the world's a stage, and all the\",\n",
    "        \"Now is the winter of our discontent\",\n",
    "        \"It was the best of times, it was the worst of times\",\n",
    "    ]\n",
    "\n",
    "    for sample in test_samples:\n",
    "        encoded = tokenizer.encode(sample)\n",
    "        decoded = tokenizer.decode(encoded)\n",
    "        assert sample == decoded, f\"Expected {sample}, got {decoded}\"\n",
    "\n",
    "test_gets_desired_vocab_size()\n",
    "test_encodes_and_decodes()\n",
    "print(\"All tests passed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CQ2. (☆☆) Complete the following code for next-token prediction\n",
    "\n",
    "In this exercise you are asked to complete parts of the lecture code, completing the training process for the Next-Token Prediction (NTP) task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import string\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformer import Transformer, SinusoidalPositionalEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell doesn't require any change, it is copied from class. It just contains data downloading, basic tokenization and the basic dataset creation.\n",
    "Note that we already covered tokenization in the first exercise, but you can also try coding this version from scratch to practice if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "DATA_DIR = \"data\"\n",
    "DATA_PATH = f\"{DATA_DIR}/shakespeare.txt\"\n",
    "\n",
    "def load_shakespeare():\n",
    "    # Download Shakespeare's works\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        response = requests.get(DATA_URL)\n",
    "        with open(DATA_PATH, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Load the text\n",
    "    with open(DATA_PATH, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "class CharTokenizer:\n",
    "    def __init__(self, text: str):\n",
    "        self.vocab = sorted(list(set(text)))\n",
    "        self.char_to_idx = {ch: idx for idx, ch in enumerate(self.vocab)}\n",
    "        self.idx_to_char = {idx: ch for idx, ch in enumerate(self.vocab)}\n",
    "        self.padding_idx = len(self.vocab)\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        return [self.char_to_idx[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, indices: list[int]):\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices if idx != self.padding_idx])\n",
    "    \n",
    "    def batch_encode(self, texts: list[str], max_len=None):\n",
    "        encoded_texts = [self.encode(text) for text in texts]\n",
    "\n",
    "        max_len_texts = max(len(text) for text in encoded_texts)\n",
    "        if max_len is None:\n",
    "            max_len = max_len_texts\n",
    "        else:\n",
    "            max_len = min(max_len, max_len_texts)\n",
    "\n",
    "        padded_texts = [\n",
    "            text + [self.padding_idx] * (max_len - len(text))\n",
    "            for text in encoded_texts\n",
    "        ]\n",
    "        attention_mask = [\n",
    "            [True] * len(text) + [False] * (max_len - len(text))\n",
    "            for text in encoded_texts\n",
    "        ]\n",
    "\n",
    "        return padded_texts, attention_mask\n",
    "    \n",
    "    def batch_decode(self, indices: list[list[int]]):\n",
    "        return [self.decode(text) for text in indices]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab) + 1  # +1 for padding\n",
    "    \n",
    "    def get_padding_idx(self):\n",
    "        return self.padding_idx\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_len):\n",
    "        self.text = text\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx, end_idx = idx * self.seq_len, (idx + 1) * self.seq_len\n",
    "        return self.text[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next is the collate function. You need to complete this function to transform the output of the tokenizer (token indices and attention mask) to torch tensors to be used during training. Bear in mind:\n",
    "1. `input_ids` and `input_attention_mask` will be fed to the model.\n",
    "2. `target_ids` are the labels, the indices of the next token at each position.\n",
    "3. `target_attention_mask` indicate which target_ids are padding and should not be predicted and taken into account when calculating the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFn:\n",
    "    def __init__(self, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # batch: List[str]\n",
    "        token_idxs, attention_mask = self.tokenizer.batch_encode(batch, self.max_seq_len)\n",
    "\n",
    "        token_idxs = torch.tensor(token_idxs)  # (batch_size, seq_len)\n",
    "        attention_mask = torch.tensor(attention_mask)  # (batch_size, seq_len)\n",
    "\n",
    "        input_ids =  # TODO\n",
    "        target_ids =  # TODO\n",
    "        input_attention_mask =  # TODO\n",
    "        target_attention_mask =  # TODO\n",
    "\n",
    "        return input_ids, target_ids, input_attention_mask, target_attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data module from Pytorch Lightning is also given to you, nothing new here to learn :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, text, tokenizer, max_seq_len, batch_size):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = TextDataset(self.text, self.max_seq_len)\n",
    "\n",
    "        train_size = int(0.9 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, val_size]\n",
    "        )\n",
    "\n",
    "        self.collate_fn = CollateFn(self.tokenizer, self.max_seq_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the LightningModule. You will need to complete it:\n",
    "1. Create a Transformer model with the given parameters, as well as the embeddings and final classification head.\n",
    "2. Code the `forward` method.\n",
    "3. Code the `_step` method, which calculates the loss function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareLightningModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Transformer params\n",
    "        vocab_size: int,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int,\n",
    "        dropout: float,\n",
    "        num_layers: int,\n",
    "        # Embedding params\n",
    "        padding_idx: int,\n",
    "        # Positional encoding params\n",
    "        max_len: int,\n",
    "        # Training params\n",
    "        optimizer_params: dict,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        # Transformer model\n",
    "        self.model = Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)\n",
    "        # self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Training params\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, input_idxs, attention_mask):\n",
    "        input_embed =  # TODO  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        _, seq_len, d_model = input_embed.size()\n",
    "\n",
    "        # NOTE Option 1: Learnable positional embeddings\n",
    "        # positions = (\n",
    "        #     torch.arange(seq_len, device=self.device)\n",
    "        #     .unsqueeze(0)\n",
    "        #     .to(self.device)\n",
    "        # )  # (1, seq_len)\n",
    "        # pos_embeddings = self.pos_embedding(positions)  # (1, seq_len, d_model)\n",
    "\n",
    "        # NOTE Option 2: Sinusoidal positional embeddings\n",
    "        pos_embeddings = (\n",
    "            SinusoidalPositionalEncoding\n",
    "            .get_positional_encoding(seq_len, d_model)\n",
    "            .to(self.device)\n",
    "            .unsqueeze(0)\n",
    "        )  # (1, seq_len, d_model)\n",
    "\n",
    "        output =  # TODO\n",
    "        return self.fc(output)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True, on_step=True)\n",
    "        return loss\n",
    "     \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "    \n",
    "    def _step(self, batch):\n",
    "        input_idxs, target_idxs, input_attention_mask, target_attention_mask = batch  # (batch_size, seq_len - 1)\n",
    "        output =  # TODO: forward pass\n",
    "\n",
    "        B, L, V = output.size()\n",
    "        loss =  # TODO: calculate the loss WITHOUT REDUCTION\n",
    "\n",
    "        # Mask out the padding tokens\n",
    "        target_attention_mask = target_attention_mask.view(-1) / target_attention_mask.sum()  # (B * L)\n",
    "        loss = (loss * target_attention_mask).sum()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.model.parameters(), **self.optimizer_params)\n",
    "        return optimizer\n",
    "    \n",
    "    def configure_callbacks(self):\n",
    "        return super().configure_callbacks() + [\n",
    "            pl.callbacks.ModelCheckpoint(monitor=\"val_loss\"),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now train the model (no changes needed). You may modify the hyperparameters to reduce resource requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_shakespeare()\n",
    "tokenizer = CharTokenizer(text)\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "padding_idx = tokenizer.get_padding_idx()\n",
    "\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "num_layers = 8\n",
    "\n",
    "max_seq_len = 100\n",
    "\n",
    "batch_size = 64\n",
    "optimizer_params = dict(lr=3e-4, weight_decay=1e-2)\n",
    "max_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = ShakespeareDataModule(text, tokenizer, max_seq_len, batch_size)\n",
    "data_module.setup()\n",
    "\n",
    "model = ShakespeareLightningModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    num_layers=num_layers,\n",
    "    padding_idx=padding_idx,\n",
    "    max_len=max_seq_len,\n",
    "    optimizer_params=optimizer_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Trainer will use only 1 of 3 GPUs because it is running inside an interactive / notebook environment. You may try to set `Trainer(devices=3)` but please note that multi-GPU inside interactive / notebook environments is considered experimental and unstable. Your mileage may vary.\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(max_epochs=max_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name      | Type        | Params\n",
      "------------------------------------------\n",
      "0 | model     | Transformer | 1.6 M \n",
      "1 | embedding | Embedding   | 8.4 K \n",
      "2 | fc        | Linear      | 8.5 K \n",
      "------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.413     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/mdl_gen/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/mdl_gen/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 157/157 [00:04<00:00, 38.80it/s, v_num=25, train_loss_step=1.300, val_loss=1.400, train_loss_epoch=1.340]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 157/157 [00:04<00:00, 38.74it/s, v_num=25, train_loss_step=1.300, val_loss=1.400, train_loss_epoch=1.340]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CQ3. (☆☆) Complete the Top-K and Top-P decoding strategies, and the `apply_temperature` method\n",
    "\n",
    "In this exercise we will code the Top-K and Top-P decoding functions seen in class. We will also learn how to apply a temperature hyperparameter to the logits to modulate how creative or model is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-K decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by programming the Top-K algorithm. If you get lost in any step, you can consult the explanation step-by-step of the method in the following cell! Just make sure to think about it a bit before giving up :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_k_decoding(logits, k=20):\n",
    "    # logits: (batch_size, vocab_size)\n",
    "    top_k_logits, top_k_indices =  # TODO: use the torch.topk function  # (batch_size, k), (batch_size, k)\n",
    "\n",
    "    # Sample from the top k indices\n",
    "    probs =  # TODO: calc probs from logits  # (batch_size, k)\n",
    "    sampled_indices = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "\n",
    "    return  # TODO: use the gather() function to select the sampled indices from top_k_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function takes two inputs:\n",
    "- `logits`: A tensor of shape `(batch_size, vocab_size)` representing unnormalized scores (logits) for each token in the vocabulary.\n",
    "- `k`: The number of top tokens to consider for sampling (default is 20).\n",
    "\n",
    "**Manual Example Input:**\n",
    "- Batch size: \\( 1 \\) (one example).\n",
    "- Vocabulary size: \\( 10 \\).\n",
    "- Top \\( k \\): \\( 3 \\).\n",
    "- Logits: `[1.0, 0.5, 2.0, -1.0, 0.0, 0.8, -0.5, 1.5, -1.5, 0.3]`.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 2: Extract the Top \\( k \\) Tokens**\n",
    "```python\n",
    "top_k_logits, top_k_indices = torch.topk(logits, k, dim=-1)\n",
    "```\n",
    "\n",
    "This extracts the \\( k \\) highest logits and their corresponding indices from the vocabulary. \n",
    "\n",
    "**Manual Example:**\n",
    "- From `logits = [1.0, 0.5, 2.0, -1.0, 0.0, 0.8, -0.5, 1.5, -1.5, 0.3]`:\n",
    "  - The top 3 logits are `[2.0, 1.5, 1.0]`.\n",
    "  - Their indices in the vocabulary are `[2, 7, 0]`.\n",
    "\n",
    "**Output:**\n",
    "- `top_k_logits = [2.0, 1.5, 1.0]` (logits of the top 3 tokens).\n",
    "- `top_k_indices = [2, 7, 0]` (indices of the top 3 tokens).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 3: Compute Probabilities for the Top \\( k \\)**\n",
    "```python\n",
    "probs = F.softmax(top_k_logits, dim=-1)\n",
    "```\n",
    "\n",
    "The logits in `top_k_logits` are converted into probabilities using the softmax function.\n",
    "\n",
    "**Manual Example:**\n",
    "Softmax computation for `top_k_logits = [2.0, 1.5, 1.0]`:\n",
    "1. Compute $e^{\\text{logit}}$ for each value:\n",
    "   $$\n",
    "   e^{2.0} = 7.389, \\, e^{1.5} = 4.482, \\, e^{1.0} = 2.718\n",
    "   $$\n",
    "2. Sum them:\n",
    "   $$\n",
    "   \\text{sum} = 7.389 + 4.482 + 2.718 = 14.589\n",
    "   $$\n",
    "3. Compute probabilities:\n",
    "   $$\n",
    "   \\text{probs} = \\left[\\frac{7.389}{14.589}, \\frac{4.482}{14.589}, \\frac{2.718}{14.589}\\right]\n",
    "               ≈ [0.506, 0.307, 0.186]\n",
    "   $$\n",
    "\n",
    "**Output:**\n",
    "- `probs = [0.506, 0.307, 0.186]` (probabilities for the top 3 tokens).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 4: Sample a Token from the Top \\( k \\)**\n",
    "```python\n",
    "sampled_indices = torch.multinomial(probs, num_samples=1)\n",
    "```\n",
    "\n",
    "The function samples one index from the `probs` distribution using multinomial sampling. \n",
    "\n",
    "**Manual Example:**\n",
    "- Sampling from `probs = [0.506, 0.307, 0.186]` could randomly select one index. Suppose the sampling picks the second index (based on the probabilities).\n",
    "\n",
    "**Output:**\n",
    "- `sampled_indices = [1]` (the sampled index within the top \\( k \\)).\n",
    "\n",
    "---\n",
    "\n",
    "#### **Step 5: Map Back to Original Vocabulary Indices**\n",
    "```python\n",
    "return top_k_indices.gather(dim=-1, index=sampled_indices)\n",
    "```\n",
    "\n",
    "The sampled index is mapped back to the original vocabulary using `top_k_indices`.\n",
    "\n",
    "**Manual Example:**\n",
    "- `top_k_indices = [2, 7, 0]`.\n",
    "- `sampled_indices = [1]` corresponds to the second element in `top_k_indices`, which is `7`.\n",
    "\n",
    "**Output:**\n",
    "- The function returns `7` as the index of the selected token in the original vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Final Output**\n",
    "For the example input, the function outputs the token index `7` as the sampled token."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top-P decoding\n",
    "\n",
    "Let's now get to programming the Top-P algorithm, which is a bit longer and less intuitive. Again, if you get lost in any step, you can consult the explanation step-by-step of the method in the following cell! Just make sure to think about it a bit before giving up :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def top_p_decoding(logits, p=0.9):\n",
    "    # logits: (batch_size, vocab_size)\n",
    "    \n",
    "    # Step 1: Sort logits and get the probabilities\n",
    "    sorted_logits, sorted_indices =  # TODO: use the torch.sort function  # (batch_size, vocab_size)\n",
    "    sorted_probs = F.softmax(sorted_logits, dim=-1)  # (batch_size, vocab_size)\n",
    "    \n",
    "    # Step 2: Compute cumulative probabilities\n",
    "    cumulative_probs = torch.cumsum(sorted_probs, dim=-1)  # (batch_size, vocab_size)\n",
    "    # TODO: understand what the previous function is doing!\n",
    "    \n",
    "    # Step 3: Mask to keep only top tokens with cumulative probability <= p\n",
    "    # We use a mask to zero out logits for tokens that don't meet the cumulative probability threshold\n",
    "    mask = cumulative_probs <= p  # (batch_size, vocab_size)\n",
    "    \n",
    "    # Ensure that we leave at least one token in the distribution\n",
    "    mask[..., 0] = 1  # Always keep at least the top token\n",
    "    \n",
    "    # Step 4: Set logits for masked tokens to a very low value (negative infinity)\n",
    "    masked_logits = # TODO: use the masked_fill() function to set all masked values to -inf  # (batch_size, vocab_size)\n",
    "    # TODO: why do you think we set the masked logits to -inf?\n",
    "    \n",
    "    # Step 5: Sample from the resulting distribution\n",
    "    probs = F.softmax(masked_logits, dim=-1)  # (batch_size, vocab_size)\n",
    "    sampled_indices = torch.multinomial(probs, num_samples=1)  # (batch_size, 1)\n",
    "    \n",
    "    # Step 6: Gather the sampled indices from the sorted list to get the actual token index\n",
    "    output_indices =  # TODO: use the gather() function to get the correct token indices from the sample  # (batch_size, 1)\n",
    "    \n",
    "    return output_indices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example:\n",
    "\n",
    "Let's assume we have the following logits for a batch of size 1 and a vocabulary size of 5. These logits represent the unnormalized prediction scores for each token:\n",
    "\n",
    "```python\n",
    "logits = torch.tensor([[1.0, 2.0, 0.5, -1.0, 0.2]])  # (batch_size=1, vocab_size=5)\n",
    "```\n",
    "\n",
    "We'll also set `p = 0.8` for this example, meaning we want to sample from the smallest set of tokens that together have a cumulative probability of at least 80%.\n",
    "\n",
    "Now, let's break down the **Top-P decoding** process for this example:\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 1: **Sort the logits and get probabilities**\n",
    "\n",
    "We first sort the logits in descending order and then convert them to probabilities using the **softmax** function.\n",
    "\n",
    "##### Sorting the logits:\n",
    "\n",
    "```python\n",
    "sorted_logits, sorted_indices = torch.sort(logits, dim=-1, descending=True)\n",
    "```\n",
    "\n",
    "This results in:\n",
    "\n",
    "```python\n",
    "sorted_logits = torch.tensor([[ 2.0,  1.0,  0.5,  0.2, -1.0]])\n",
    "sorted_indices = torch.tensor([[1, 0, 2, 4, 3]])\n",
    "```\n",
    "\n",
    "- The logits are now sorted in descending order: `[2.0, 1.0, 0.5, 0.2, -1.0]`.\n",
    "- `sorted_indices` is a tensor that contains the indices of the sorted tokens in the original order: `[1, 0, 2, 4, 3]`.\n",
    "\n",
    "Next, apply the **softmax** to convert logits into probabilities:\n",
    "\n",
    "```python\n",
    "sorted_probs = F.softmax(sorted_logits, dim=-1)\n",
    "```\n",
    "\n",
    "This gives us the probabilities:\n",
    "\n",
    "```python\n",
    "sorted_probs = torch.tensor([[0.4625, 0.2555, 0.1320, 0.1072, 0.0428]])\n",
    "```\n",
    "\n",
    "- **Softmax** ensures that the values sum to 1. In this case, we get: `[0.4625, 0.2555, 0.1320, 0.1072, 0.0428]`.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 2: **Compute cumulative probabilities**\n",
    "\n",
    "Now, we calculate the **cumulative sum** of the sorted probabilities. This helps us track how much probability mass we are accumulating as we consider the top tokens.\n",
    "\n",
    "```python\n",
    "cumulative_probs = torch.cumsum(sorted_probs, dim=-1)\n",
    "```\n",
    "\n",
    "The cumulative probabilities are:\n",
    "\n",
    "```python\n",
    "cumulative_probs = torch.tensor([[0.4625, 0.7180, 0.8500, 0.9572, 1.0000]])\n",
    "```\n",
    "\n",
    "- The cumulative probabilities represent how much of the total probability mass is covered by the top `n` tokens.\n",
    "  - After the top 1 token, the cumulative probability is `0.4625`.\n",
    "  - After the top 2 tokens, the cumulative probability is `0.7180`.\n",
    "  - After the top 3 tokens, the cumulative probability is `0.8500`, and so on.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 3: **Create a mask to keep tokens where cumulative probability $\\leq p$**\n",
    "\n",
    "Now, we create a **mask** that selects tokens where the cumulative probability is less than or equal to `p`. This ensures that we only sample from the smallest subset of tokens that add up to at least `p` probability.\n",
    "\n",
    "For `p = 0.8`, we create the mask:\n",
    "\n",
    "```python\n",
    "mask = cumulative_probs <= p\n",
    "```\n",
    "\n",
    "This gives us:\n",
    "\n",
    "```python\n",
    "mask = torch.tensor([[True, True, True, False, False]])\n",
    "```\n",
    "\n",
    "- The top 3 tokens (with cumulative probability `0.8500`) are included because their cumulative probability is less than or equal to `0.8`.\n",
    "- The mask ensures that only the top 3 tokens are considered for sampling.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 4: **Set logits of masked-out tokens to a very low value**\n",
    "\n",
    "Now that we know which tokens should be considered (from the mask), we **mask out** the logits of the tokens that we don't want to consider. We do this by setting their logits to a very low value (negative infinity), so their corresponding probabilities become zero.\n",
    "\n",
    "```python\n",
    "masked_logits = sorted_logits.masked_fill(mask == 0, float('-inf'))\n",
    "```\n",
    "\n",
    "This results in:\n",
    "\n",
    "```python\n",
    "masked_logits = torch.tensor([[ 2.0,  1.0,  0.5, -inf, -inf]])\n",
    "```\n",
    "\n",
    "- The logits for tokens that are masked out (`-inf`) are effectively removed from the probability distribution.\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 5: **Reapply softmax to get a valid distribution and sample**\n",
    "\n",
    "After masking out the irrelevant tokens, we apply the **softmax** function again to get a valid probability distribution over the remaining tokens.\n",
    "\n",
    "```python\n",
    "probs = F.softmax(masked_logits, dim=-1)\n",
    "```\n",
    "\n",
    "This gives us the new probabilities:\n",
    "\n",
    "```python\n",
    "probs = torch.tensor([[0.4625, 0.2555, 0.1320, 0.0000, 0.0000]])\n",
    "```\n",
    "\n",
    "- Now the probabilities only sum over the top 3 tokens (`[0.4625, 0.2555, 0.1320]`), and the others are zero.\n",
    "\n",
    "We can now **sample** from this distribution:\n",
    "\n",
    "```python\n",
    "sampled_indices = torch.multinomial(probs, num_samples=1)\n",
    "```\n",
    "\n",
    "Suppose the sampled index is `0`, meaning we select the token corresponding to the first index in the sorted list, which is `1` in the original vocabulary (corresponding to the `2.0` logit).\n",
    "\n",
    "---\n",
    "\n",
    "#### Step 6: **Map back to the original token indices**\n",
    "\n",
    "Remember that `sorted_indices = [1, 0, 2, 4, 3]`, which meant that the most likely token was $t_1$, the second most likely token was $t_0$, the third was $t_2$, and so on. When we sampled in the previous step, we were sampling with the sorted probabilities:\n",
    "- Sampling a $0$ means that we must take the most likely token, which in our example is $t_1$.\n",
    "- Sampling a $4$ would mean taking the fifth most likely token, which in our example is $t_3$.\n",
    "\n",
    "Thus, we need to **map** the sampled index back to the original token index. We use the `sorted_indices` tensor to get the correct token.\n",
    "\n",
    "```python\n",
    "output_indices = torch.gather(sorted_indices, dim=-1, index=sampled_indices)\n",
    "```\n",
    "\n",
    "This gives us:\n",
    "\n",
    "```python\n",
    "output_indices = torch.tensor([[1]])\n",
    "```\n",
    "\n",
    "This means we sampled the token corresponding to index `1` in the original vocabulary, which is the token with the logit `2.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "def test_top_p_decoding():\n",
    "    torch.manual_seed(42)  # Set seed for reproducibility\n",
    "\n",
    "    batch_size, vocab_size = 10, 20\n",
    "    logits = torch.randn(batch_size, vocab_size)\n",
    "\n",
    "    sample = top_p_decoding(logits)\n",
    "    expected_sample = torch.tensor([\n",
    "        [ 2], [ 1], [ 5], [11], [14],\n",
    "        [ 4], [ 8], [ 7], [10], [ 8]\n",
    "    ])\n",
    "\n",
    "    assert torch.all(sample == expected_sample), f\"Expected {expected_sample}, got {sample}\"\n",
    "\n",
    "test_top_p_decoding()\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_temperature(logits, temperature=0.7):\n",
    "    # Apply temperature scaling\n",
    "    return  # TODO: apply temperature scaling to the logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All tests passed!\n"
     ]
    }
   ],
   "source": [
    "# Create a test\n",
    "def test_apply_temperature():\n",
    "    torch.manual_seed(42)  # Set seed for reproducibility\n",
    "\n",
    "    logits = torch.tensor([[1.0, 2.0, 3.0]])\n",
    "    temperature = 0.5\n",
    "\n",
    "    output = apply_temperature(logits, temperature)\n",
    "    expected_output = torch.tensor([[2.0, 4.0, 6.0]])\n",
    "\n",
    "    assert torch.all(output == expected_output), f\"Expected {expected_output}, got {output}\"\n",
    "\n",
    "test_apply_temperature()\n",
    "print(\"All tests passed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at how the temperature parameter affects the probability distribution!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x400 with 5 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logits = torch.randn(1, 5)\n",
    "probs = F.softmax(logits, dim=-1).numpy()\n",
    "\n",
    "temperatures = [0.1, 0.5, 1.0, 2.0, 5.0]\n",
    "scaled_probs = [F.softmax(apply_temperature(logits, temp), dim=-1).numpy() for temp in temperatures]\n",
    "\n",
    "# Bar plot for each temperature separately\n",
    "fig, ax = plt.subplots(1, 5, figsize=(20, 4))\n",
    "for i, temp in enumerate(temperatures):\n",
    "    ax[i].bar(range(5), scaled_probs[i][0])\n",
    "    ax[i].set_title(f\"Temperature: {temp}\")\n",
    "    ax[i].set_xticks(range(5))\n",
    "    ax[i].set_ylim([0, 1])\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
