{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 2 - Exercise notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CQ1. (â˜†) Create upsampling convolutional models to match a target shape from the input shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are tasked with designing neural network modules that upsample a given input tensor to match a specific target shape using transposed convolutional layers (`nn.ConvTranspose2d`). Your goal is to carefully configure the parameters of each layer, including kernel size, stride, padding, and output padding, to achieve the desired output dimensions.\n",
    "\n",
    "#### Instructions:\n",
    "\n",
    "1. **Input Tensor**: Each exercise provides an input tensor with a specific shape.\n",
    "2. **Target Output Shape**: Your module must upsample the input to match a given target shape.\n",
    "3. **Create a Module**: Define a sequence of transposed convolutional layers using the provided `create_module` function with `nn.ConvTranspose2d` layers to achieve the target shape. **NOTE**: you can do each exercise in an infinite number of ways, with any number of layers.\n",
    "4. **Shape Validation**: After applying the module to the input tensor, ensure the output matches the target shape by using the provided `check_solution` function.\n",
    "5. **Debugging with Hooks**: The `create_module` adds hooks to print the intermediate shapes after each layer to understand the transformations.\n",
    "\n",
    "#### Example Problem:\n",
    "\n",
    "```python\n",
    "# Example Input Tensor\n",
    "input = torch.randn(1, 3, 8, 8)\n",
    "\n",
    "# Target Output Shape: (1, 16, 32, 32)\n",
    "module = create_module([\n",
    "    nn.ConvTranspose2d(3, 8, kernel_size=3, stride=2, padding=1),\n",
    "    nn.ConvTranspose2d(8, 16, kernel_size=3, stride=2, padding=0, output_padding=1),\n",
    "])\n",
    "output = module(input)\n",
    "\n",
    "# Check if the output matches the target shape\n",
    "check_solution((1, 16, 32, 32), output.shape)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_solution(expected_shape, output_shape):\n",
    "    print(\"=\" * 80)\n",
    "    if output_shape != expected_shape:\n",
    "        print(f\"Failure :(. Expected output shape ${expected_shape}, but got {output_shape}\")\n",
    "    else:\n",
    "        print(\"Success!\")\n",
    "    print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_module(layers):\n",
    "    module = nn.Sequential(*layers)\n",
    "    \n",
    "    # Hook to print the output shape after each layer\n",
    "    def hook(module, input, output):\n",
    "        print(f\"Output shape after {module.__class__.__name__}: {output.shape}\")\n",
    "    \n",
    "    # Register the hook for each layer\n",
    "    for layer in module:\n",
    "        if isinstance(layer, nn.ConvTranspose2d):\n",
    "            layer.register_forward_hook(hook)\n",
    "    \n",
    "    return module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after ConvTranspose2d: torch.Size([1, 8, 15, 15])\n",
      "Output shape after ConvTranspose2d: torch.Size([1, 16, 32, 32])\n",
      "================================================================================\n",
      "Success!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exercise 1\n",
    "input = torch.randn(1, 3, 8, 8)\n",
    "module = create_module([\n",
    "    # TODO: complete\n",
    "])\n",
    "output = module(input)\n",
    "check_solution((1, 16, 32, 32), output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after ConvTranspose2d: torch.Size([1, 8, 32, 32])\n",
      "================================================================================\n",
      "Success!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exercise 2\n",
    "input = torch.randn(1, 1, 16, 16)\n",
    "module = create_module([\n",
    "    # TODO: complete\n",
    "])\n",
    "output = module(input)\n",
    "check_solution((1, 8, 32, 32), output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after ConvTranspose2d: torch.Size([1, 10, 24, 24])\n",
      "================================================================================\n",
      "Success!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exercise 3\n",
    "input = torch.randn(1, 2, 8, 8)\n",
    "module = create_module([\n",
    "    # TODO: complete\n",
    "])\n",
    "output = module(input)\n",
    "check_solution((1, 10, 24, 24), output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after ConvTranspose2d: torch.Size([1, 24, 21, 21])\n",
      "================================================================================\n",
      "Success!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exercise 4\n",
    "input = torch.randn(1, 4, 10, 10)\n",
    "module = create_module([\n",
    "    # TODO: complete\n",
    "])\n",
    "output = module(input)\n",
    "check_solution((1, 24, 21, 21), output.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after ConvTranspose2d: torch.Size([1, 8, 16, 16])\n",
      "Output shape after ConvTranspose2d: torch.Size([1, 16, 32, 32])\n",
      "================================================================================\n",
      "Success!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exercise 5\n",
    "input = torch.randn(1, 5, 7, 7)\n",
    "module = create_module([\n",
    "    # TODO: complete\n",
    "])\n",
    "output = module(input)\n",
    "check_solution((1, 16, 32, 32), output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after ConvTranspose2d: torch.Size([1, 24, 26, 26])\n",
      "================================================================================\n",
      "Success!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exercise 6\n",
    "input = torch.randn(1, 6, 9, 9)\n",
    "module = create_module([\n",
    "    # TODO: complete\n",
    "])\n",
    "output = module(input)\n",
    "check_solution((1, 24, 26, 26), output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after ConvTranspose2d: torch.Size([1, 10, 13, 13])\n",
      "Output shape after ConvTranspose2d: torch.Size([1, 20, 40, 40])\n",
      "================================================================================\n",
      "Success!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exercise 7\n",
    "input = torch.randn(1, 3, 6, 6)\n",
    "module = create_module([\n",
    "    # TODO: complete\n",
    "])\n",
    "output = module(input)\n",
    "check_solution((1, 20, 40, 40), output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after ConvTranspose2d: torch.Size([1, 5, 15, 15])\n",
      "Output shape after ConvTranspose2d: torch.Size([1, 10, 29, 29])\n",
      "Output shape after ConvTranspose2d: torch.Size([1, 15, 31, 31])\n",
      "================================================================================\n",
      "Success!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exercise 8\n",
    "input = torch.randn(1, 1, 8, 8)\n",
    "module = create_module([\n",
    "    # TODO: complete\n",
    "])\n",
    "output = module(input)\n",
    "check_solution((1, 15, 31, 31), output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after ConvTranspose2d: torch.Size([1, 6, 24, 24])\n",
      "Output shape after ConvTranspose2d: torch.Size([1, 24, 50, 50])\n",
      "================================================================================\n",
      "Success!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exercise 9\n",
    "input = torch.randn(1, 2, 12, 12)\n",
    "module = create_module([\n",
    "    # TODO: complete\n",
    "])\n",
    "output = module(input)\n",
    "check_solution((1, 24, 50, 50), output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape after ConvTranspose2d: torch.Size([1, 16, 22, 22])\n",
      "Output shape after ConvTranspose2d: torch.Size([1, 64, 44, 44])\n",
      "================================================================================\n",
      "Success!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Exercise 10\n",
    "input = torch.randn(1, 8, 10, 10)\n",
    "module = create_module([\n",
    "    # TODO: complete\n",
    "])\n",
    "output = module(input)\n",
    "check_solution((1, 64, 44, 44), output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CQ2. (â˜†â˜†) Code the VAE seen in class from scratch, on a dataset of your choosing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select an image dataset that you like, and based on the input size of the images complete the following VAE code seen in class. Check that the model trains and that you can generate new samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, latent_dim=256, activation_cls=nn.LeakyReLU):\n",
    "        super(VAE, self).__init__()\n",
    "\n",
    "        # Encoder\n",
    "        def get_encoder_block(input_channels, output_channels):\n",
    "            return [\n",
    "                nn.Conv2d(input_channels, output_channels, kernel_size=3, stride=2, padding=1),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                activation_cls(),\n",
    "            ]\n",
    "\n",
    "        self.encoder = nn.Sequential(\n",
    "            # TODO: Add the encoder blocks\n",
    "            nn.Flatten(),  # 512x2x2 -> 2048\n",
    "        )\n",
    "\n",
    "        # Latent space\n",
    "        self.fc_mu = nn.Linear(, latent_dim)  # TODO: complete the input size\n",
    "        self.fc_logvar = nn.Linear(, latent_dim)  # TODO: complete the input size\n",
    "\n",
    "        # Decoder\n",
    "        def get_decoder_block(input_channels, output_channels):\n",
    "            return [\n",
    "                nn.ConvTranspose2d(input_channels, output_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "                nn.BatchNorm2d(output_channels),\n",
    "                activation_cls(),\n",
    "            ]\n",
    "\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, ),  # TODO: complete the output size\n",
    "            activation_cls(),\n",
    "            nn.Unflatten(1, ),  # TODO: Unflatten the tensor\n",
    "            # TODO: Add the decoder blocks\n",
    "            nn.Conv2d(16, 3, kernel_size=3, stride=1, padding=1),  # 16x64x64 -> 3x64x64\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        mu = self.fc_mu(encoded)\n",
    "        logvar = self.fc_logvar(encoded)\n",
    "        z = self.__reparameterize(mu, logvar)\n",
    "        decoded = self.decoder(z)\n",
    "        return decoded, mu, logvar\n",
    "\n",
    "    def __reparameterize(self, mu, logvar):\n",
    "        # TODO: Implement the reparameterization trick\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAELightningModule(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        latent_dim: int = 256,\n",
    "        optimizer_params: dict = None,\n",
    "        beta_kl: float = 1.,\n",
    "    ):\n",
    "        super(VAELightningModule, self).__init__()\n",
    "        self.save_hyperparameters()\n",
    "        self.model = VAE(latent_dim=latent_dim)\n",
    "        self.optimizer_params = optimizer_params\n",
    "        self.beta_kl = beta_kl\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        # TODO: the following lane might need to be changed depending on the dataset\n",
    "        x, _ = batch\n",
    "        recon_batch, mu, logvar = self.model(x)\n",
    "        loss, mse, kld = self.__loss_fn(recon_batch, x, mu, logvar)\n",
    "        self.log_dict(\n",
    "            {'train_mse_step': mse, 'train_kld_step': kld, 'train_loss_step': loss}, \n",
    "            on_step=True, on_epoch=False, prog_bar=True, logger=False\n",
    "        )\n",
    "        self.log_dict(\n",
    "            {'train_mse': mse, 'train_kld': kld, 'train_loss': loss},\n",
    "            on_step=False, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # TODO: the following lane might need to be changed depending on the dataset\n",
    "        x, _ = batch\n",
    "        recon_batch, mu, logvar = self.model(x)\n",
    "        loss, _, _ = self.__loss_fn(recon_batch, x, mu, logvar)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True, logger=True)\n",
    "        return loss\n",
    "\n",
    "    def __loss_fn(self, recon_x, x, mu, logvar):\n",
    "        # TODO: complete\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.model.parameters(), **self.optimizer_params)\n",
    "        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=20, eta_min=1e-6)\n",
    "        return {\n",
    "            'optimizer': optimizer,\n",
    "            'lr_scheduler': {\n",
    "                'scheduler': scheduler,\n",
    "                'interval': 'step',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def configure_callbacks(self):\n",
    "        return [\n",
    "            pl.callbacks.ModelCheckpoint(monitor='val_loss'),\n",
    "            pl.callbacks.EarlyStopping(monitor='val_loss', patience=6, mode='min'),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LEARNING_RATE = 1e-3\n",
    "WEIGHT_DECAY = 0.\n",
    "MAX_EPOCHS = 50\n",
    "BETA_KL = 1. / 200.  # to weight the KL term in the loss\n",
    "LATENT_DIM = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer_params = {\n",
    "    'lr': LEARNING_RATE,\n",
    "    'weight_decay': WEIGHT_DECAY,\n",
    "}\n",
    "module = VAELightningModule(optimizer_params=optimizer_params, beta_kl=BETA_KL)\n",
    "data_module =  # TODO: complete\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = pl.Trainer(\n",
    "    accelerator='gpu',  # comment this line if you don't have a GPU!\n",
    "    devices=[0],  # comment this line if you don't have a GPU!\n",
    "    max_epochs=MAX_EPOCHS,\n",
    "    precision=\"32\",\n",
    "    gradient_clip_val=1.0,\n",
    ")\n",
    "trainer.fit(module, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CQ3. (â˜†â˜†) Modify the VAE code to create a conditional VAE for MNIST."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To-Do as part of the evaluation!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
