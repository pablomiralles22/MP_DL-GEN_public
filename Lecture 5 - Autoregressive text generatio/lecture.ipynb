{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import pytorch_lightning as pl\n",
    "import string\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "from transformer import Transformer, SinusoidalPositionalEncoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We begin by downloading the dataset (credit to Andrej Karpathy) to the `data/shakespeare.txt` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_URL = \"https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\"\n",
    "DATA_DIR = \"data\"\n",
    "DATA_PATH = f\"{DATA_DIR}/shakespeare.txt\"\n",
    "\n",
    "def load_shakespeare():\n",
    "    # Download Shakespeare's works\n",
    "    os.makedirs(DATA_DIR, exist_ok=True)\n",
    "    if not os.path.exists(DATA_PATH):\n",
    "        response = requests.get(DATA_URL)\n",
    "        with open(DATA_PATH, 'wb') as f:\n",
    "            f.write(response.content)\n",
    "    \n",
    "    # Load the text\n",
    "    with open(DATA_PATH, 'r') as f:\n",
    "        text = f.read()\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is a single `.txt` file with text from Shakespeare. Let's now create a very basic tokenizer, that maps each character to a different token:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharTokenizer:\n",
    "    def __init__(self, text: str):\n",
    "        self.vocab = sorted(list(set(text)))\n",
    "        self.char_to_idx = {ch: idx for idx, ch in enumerate(self.vocab)}\n",
    "        self.idx_to_char = {idx: ch for idx, ch in enumerate(self.vocab)}\n",
    "        self.padding_idx = len(self.vocab)\n",
    "\n",
    "    def encode(self, text: str):\n",
    "        return [self.char_to_idx[ch] for ch in text]\n",
    "    \n",
    "    def decode(self, indices: list[int]):\n",
    "        return ''.join([self.idx_to_char[idx] for idx in indices if idx != self.padding_idx])\n",
    "    \n",
    "    def batch_encode(self, texts: list[str], max_len=None):\n",
    "        encoded_texts = [self.encode(text) for text in texts]\n",
    "\n",
    "        max_len_texts = max(len(text) for text in encoded_texts)\n",
    "        if max_len is None:\n",
    "            max_len = max_len_texts\n",
    "        else:\n",
    "            max_len = min(max_len, max_len_texts)\n",
    "\n",
    "        padded_texts = [\n",
    "            text + [self.padding_idx] * (max_len - len(text))\n",
    "            for text in encoded_texts\n",
    "        ]\n",
    "        attention_mask = [\n",
    "            [True] * len(text) + [False] * (max_len - len(text))\n",
    "            for text in encoded_texts\n",
    "        ]\n",
    "\n",
    "        return padded_texts, attention_mask\n",
    "    \n",
    "    def batch_decode(self, indices: list[list[int]]):\n",
    "        return [self.decode(text) for text in indices]\n",
    "    \n",
    "    def get_vocab_size(self):\n",
    "        return len(self.vocab) + 1  # +1 for padding\n",
    "    \n",
    "    def get_padding_idx(self):\n",
    "        return self.padding_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's explain the code step by step. This code defines a Python class `CharTokenizer` that is responsible for converting text into sequences of character indices and vice versa, as well as handling padding and batching for processing multiple texts.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Constructor: `__init__`**\n",
    "\n",
    "```python\n",
    "def __init__(self, text: str):\n",
    "    self.vocab = sorted(list(set(text)))\n",
    "    self.char_to_idx = {ch: idx for idx, ch in enumerate(self.vocab)}\n",
    "    self.idx_to_char = {idx: ch for idx, ch in enumerate(self.vocab)}\n",
    "    self.padding_idx = len(self.vocab)\n",
    "```\n",
    "\n",
    "- **`__init__(self, text: str)`**: This is the constructor method that gets called when an instance of the class is created. It initializes the object with a `text` string.\n",
    "  \n",
    "- **`self.vocab = sorted(list(set(text)))`**:\n",
    "  - `set(text)` converts the `text` string into a set of unique characters.\n",
    "  - `list(set(text))` turns the set into a list, which is then sorted to create an ordered list of unique characters, forming the vocabulary.\n",
    "\n",
    "- **`self.char_to_idx = {ch: idx for idx, ch in enumerate(self.vocab)}`**:\n",
    "  - This dictionary comprehension creates a mapping of characters (`ch`) to their corresponding indices (`idx`).\n",
    "  - For example, if the vocabulary is `['a', 'b', 'c']`, the dictionary would be `{'a': 0, 'b': 1, 'c': 2}`.\n",
    "\n",
    "- **`self.idx_to_char = {idx: ch for idx, ch in enumerate(self.vocab)}`**:\n",
    "  - This dictionary comprehension does the reverse mapping, creating a dictionary that maps indices to characters.\n",
    "  - For example, `idx_to_char = {0: 'a', 1: 'b', 2: 'c'}`.\n",
    "\n",
    "- **`self.padding_idx = len(self.vocab)`**:\n",
    "  - The padding index is assigned the value of the length of the vocabulary.\n",
    "  - This means that the padding character will have an index one greater than the highest index of any character in the vocabulary.\n",
    "  - The padding token is useful when we encode texts of different lengths in a single batch. Since all of the sequences in a tensor must have the same length, the shorter ones are padded to match length of the largest ones, with this padding token.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Method: `encode`**\n",
    "\n",
    "```python\n",
    "def encode(self, text: str):\n",
    "    return [self.char_to_idx[ch] for ch in text]\n",
    "```\n",
    "\n",
    "- **`encode(self, text: str)`**: This method takes a string `text` and returns a list of indices corresponding to each character in the string based on the `char_to_idx` dictionary.\n",
    "  \n",
    "- For example, if `text = \"abc\"`, it will return `[0, 1, 2]` (based on the vocab `['a', 'b', 'c']`).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Method: `decode`**\n",
    "\n",
    "```python\n",
    "def decode(self, indices: list[int]):\n",
    "    return ''.join([self.idx_to_char[idx] for idx in indices if idx != self.padding_idx])\n",
    "```\n",
    "\n",
    "- **`decode(self, indices: list[int])`**: This method takes a list of character indices and converts them back into a string of characters.\n",
    "  \n",
    "- **`''.join([...])`**: Joins the list of characters into a single string.\n",
    "  \n",
    "- **`if idx != self.padding_idx`**: This ensures that if there are any padding indices (those with value `self.padding_idx`), they are ignored during the decoding process.\n",
    "\n",
    "- For example, if `indices = [0, 1, 2]` and the vocab is `['a', 'b', 'c']`, it will return `\"abc\"`. If the list contains padding indices like `[0, 1, 2, 4]` (where 4 is the padding index), it will ignore the padding index and return `\"abc\"`.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Method: `batch_encode`**\n",
    "\n",
    "```python\n",
    "def batch_encode(self, texts: list[str], max_len=None):\n",
    "    encoded_texts = [self.encode(text) for text in texts]\n",
    "\n",
    "    max_len_texts = max(len(text) for text in encoded_texts)\n",
    "    if max_len is None:\n",
    "        max_len = max_len_texts\n",
    "    else:\n",
    "        max_len = min(max_len, max_len_texts)\n",
    "\n",
    "    padded_texts = [\n",
    "        text + [self.padding_idx] * (max_len - len(text))\n",
    "        for text in encoded_texts\n",
    "    ]\n",
    "    attention_mask = [\n",
    "        [True] * len(text) + [False] * (max_len - len(text))\n",
    "        for text in encoded_texts\n",
    "    ]\n",
    "\n",
    "    return padded_texts, attention_mask\n",
    "```\n",
    "\n",
    "- **`batch_encode(self, texts: list[str], max_len=None)`**: This method takes a list of strings (`texts`) and encodes them into a batch of padded sequences. Optionally, a `max_len` can be provided to specify the maximum length of the padded sequences.\n",
    "\n",
    "- **`encoded_texts = [self.encode(text) for text in texts]`**: Each string in the `texts` list is encoded into a list of indices using the `encode` method.\n",
    "\n",
    "- **`max_len_texts = max(len(text) for text in encoded_texts)`**: This computes the maximum length of all encoded texts in the batch.\n",
    "\n",
    "- **`if max_len is None:`**:\n",
    "  - If `max_len` is not provided, the method uses `max_len_texts` (the length of the longest encoded text).\n",
    "  - If `max_len` is provided, it will be used, but the method will take the smaller of `max_len` and `max_len_texts` to avoid unnecessary length.\n",
    "\n",
    "- **`padded_texts = [...]`**:\n",
    "  - For each encoded text, it pads the text with the `padding_idx` (using list comprehension) until the length reaches `max_len`.\n",
    "\n",
    "- **`attention_mask = [...]`**:\n",
    "  - This creates a corresponding attention mask for each padded text.\n",
    "  - `True` values correspond to actual characters, and `False` values correspond to padding positions.\n",
    "\n",
    "- **Return**:\n",
    "  - The method returns two lists:\n",
    "    - `padded_texts`: A list of padded character sequences.\n",
    "    - `attention_mask`: A list of masks, indicating which positions are actual characters and which are padding.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Method: `batch_decode`**\n",
    "\n",
    "```python\n",
    "def batch_decode(self, indices: list[list[int]]):\n",
    "    return [self.decode(text) for text in indices]\n",
    "```\n",
    "\n",
    "- **`batch_decode(self, indices: list[list[int]])`**: This method takes a batch of encoded texts (a list of lists of indices) and decodes each one using the `decode` method.\n",
    "\n",
    "- It returns a list of decoded strings, one for each list of indices in the input batch.\n",
    "\n",
    "---\n",
    "\n",
    "### 6. **Method: `get_vocab_size`**\n",
    "\n",
    "```python\n",
    "def get_vocab_size(self):\n",
    "    return len(self.vocab) + 1  # +1 for padding\n",
    "```\n",
    "\n",
    "- **`get_vocab_size(self)`**: This method returns the size of the vocabulary, including the padding token (which has its own index).\n",
    "  - The length of `self.vocab` gives the number of unique characters in the vocabulary.\n",
    "  - `+1` accounts for the padding token, which increases the vocabulary size.\n",
    "\n",
    "---\n",
    "\n",
    "### 7. **Method: `get_padding_idx`**\n",
    "\n",
    "```python\n",
    "def get_padding_idx(self):\n",
    "    return self.padding_idx\n",
    "```\n",
    "\n",
    "- **`get_padding_idx(self)`**: This method simply returns the index reserved for padding.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary of the Class\n",
    "\n",
    "The `CharTokenizer` class provides the following functionality:\n",
    "- **Tokenization**: Converts text into a sequence of character indices (`encode`).\n",
    "- **Detokenization**: Converts sequences of indices back into text, ignoring padding (`decode`).\n",
    "- **Batch Processing**: Handles multiple texts at once, encoding them, padding them to a fixed length, and creating attention masks for each sequence (`batch_encode` and `batch_decode`).\n",
    "- **Padding**: Handles padding by assigning a special index to padding tokens (`padding_idx`).\n",
    "- **Vocabulary Information**: Provides the size of the vocabulary and the padding index (`get_vocab_size` and `get_padding_idx`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build the dataset, which simply takes the source text and the sequence length. The class basically breaks down the text into chunks of the size provided during initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, text, seq_len):\n",
    "        self.text = text\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text) // self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        start_idx, end_idx = idx * self.seq_len, (idx + 1) * self.seq_len\n",
    "        return self.text[start_idx:end_idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We notice two special things in this toy dataset, which will not be present in a real world scenario:\n",
    "- We have only one text, and all chunks will be of the same size, except perhaps the last one. This makes padding and masking less important (we could drop the last chunk and have all samples with the same length). However, in real datasets we have multiple texts, each with a different length.\n",
    "- We also find that with a single text, it doesn't really make sense to add a special token `End of Sequence`, since it would only appear once in the dataset. **This means that the model is not learning how to stop generating on its own**. Thus, we cap the generation process with a maximum number of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now create a `CollateFn` function to prepare samples for our training procedure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFn:\n",
    "    def __init__(self, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        # batch: List[str]\n",
    "        token_idxs, attention_mask = self.tokenizer.batch_encode(batch, self.max_seq_len)\n",
    "\n",
    "        token_idxs = torch.tensor(token_idxs)  # (batch_size, seq_len)\n",
    "        attention_mask = torch.tensor(attention_mask)  # (batch_size, seq_len)\n",
    "\n",
    "        input_ids = token_idxs[:, :-1]  # (batch_size, seq_len - 1)\n",
    "        target_ids = token_idxs[:, 1:]  # (batch_size, seq_len - 1)\n",
    "        input_attention_mask = attention_mask[:, :-1]  # (batch_size, seq_len - 1)\n",
    "        target_attention_mask = attention_mask[:, 1:]  # (batch_size, seq_len - 1)\n",
    "\n",
    "        return input_ids, target_ids, input_attention_mask, target_attention_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **Class Definition: `CollateFn`**\n",
    "\n",
    "```python\n",
    "class CollateFn:\n",
    "    def __init__(self, tokenizer, max_seq_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "```\n",
    "\n",
    "- **`CollateFn`**: This class defines a custom collate function. A collate function is responsible for batching and transforming raw data into a form that the model can process.\n",
    "- **`__init__(self, tokenizer, max_seq_len)`**: This is the constructor method that initializes the `CollateFn` class.\n",
    "  - **`tokenizer`**: This is an instance of a tokenizer, which is used to convert text into token indices.\n",
    "  - **`max_seq_len`**: This parameter specifies the maximum sequence length for each input text after padding/truncation. If any input text exceeds this length, it will be truncated.\n",
    "\n",
    "---\n",
    "\n",
    "2. **Calling the Collate Function**\n",
    "\n",
    "```python\n",
    "def __call__(self, batch):\n",
    "```\n",
    "\n",
    "- **`__call__(self, batch)`**: This is the function that gets called when the collate function is used. It takes a `batch` as input, which is a list of strings (text data).\n",
    "\n",
    "---\n",
    "\n",
    "3. **Batch Encoding and Padding**\n",
    "\n",
    "```python\n",
    "token_idxs, attention_mask = self.tokenizer.batch_encode(batch, self.max_seq_len)\n",
    "```\n",
    "\n",
    "- **`batch_encode`**:\n",
    "  - The `batch_encode` method of the tokenizer is called to encode the list of text data (`batch`) into token indices.\n",
    "  - **`batch`**: This is a list of strings, where each string is a sentence or text.\n",
    "  - **`max_seq_len`**: The maximum sequence length after padding/truncation.\n",
    "  \n",
    "- **Output**: \n",
    "  - `token_idxs`: A list of token indices for each sentence in the batch. This represents the input tokens in numerical form.\n",
    "  - `attention_mask`: A mask that indicates which tokens are actual tokens (True) and which are padding (False). It helps the model ignore padded tokens.\n",
    "\n",
    "---\n",
    "\n",
    "4. **Convert to Tensors**\n",
    "\n",
    "```python\n",
    "token_idxs = torch.tensor(token_idxs)  # (batch_size, seq_len)\n",
    "attention_mask = torch.tensor(attention_mask)  # (batch_size, seq_len)\n",
    "```\n",
    "\n",
    "- **`torch.tensor(token_idxs)`**: Converts the list of token indices into a PyTorch tensor of shape `(batch_size, seq_len)`. `batch_size` is the number of sentences in the batch, and `seq_len` is the length of the token sequence (after padding/truncation).\n",
    "  \n",
    "- **`torch.tensor(attention_mask)`**: Converts the list of attention masks into a tensor of shape `(batch_size, seq_len)`. Each entry in the attention mask tensor will be `True` (1) for actual tokens and `False` (0) for padding tokens.\n",
    "\n",
    "---\n",
    "\n",
    "5. **Create Input and Target Sequences for Next-Token Prediction**\n",
    "\n",
    "```python\n",
    "input_ids = token_idxs[:, :-1]  # (batch_size, seq_len - 1)\n",
    "target_ids = token_idxs[:, 1:]  # (batch_size, seq_len - 1)\n",
    "attention_mask = attention_mask[:, :-1]  # (batch_size, seq_len - 1)\n",
    "target_attention_mask = attention_mask[:, 1:]  # (batch_size, seq_len - 1)\n",
    "```\n",
    "\n",
    "- **`input_ids = token_idxs[:, :-1]`**:\n",
    "  - This slices the `token_idxs` tensor to remove the last token from each sequence in the batch.\n",
    "  - The `input_ids` represent the input tokens used to predict the next token. For example, given a sequence `[a, b, c]`, the `input_ids` will be `[a, b]`.\n",
    "  - The shape of `input_ids` is `(batch_size, seq_len - 1)`.\n",
    "- **`target_ids = token_idxs[:, 1:]`**:\n",
    "  - This slices the `token_idxs` tensor to remove the first token from each sequence in the batch.\n",
    "  - The `target_ids` represent the \"true\" next token for the model to predict. For example, given a sequence `[a, b, c]`, the `target_ids` will be `[b, c]`.\n",
    "  - The shape of `target_ids` is also `(batch_size, seq_len - 1)`.\n",
    "- **`input_attention_mask = attention_mask[:, :-1]`**:\n",
    "  - This slices the `attention_mask` to match the length of the `input_ids` (i.e., removing the last token's attention mask).\n",
    "  - This mask will be used when passing the input tokens through the model.\n",
    "  - The resulting `attention_mask` tensor will have shape `(batch_size, seq_len - 1)` and will indicate which tokens are actual tokens (True) and which are padding (False).\n",
    "- **`target_attention_mask = attention_mask[:, 1:]`**:\n",
    "  - This slices the `attention_mask` to match the length of the `target_input_ids` (i.e., removing the first token's attention mask).\n",
    "  - This mask will be used when calculating the final loss. Since padding tokens should not be predicted, we must be able to remove them from the loss!\n",
    "  - The resulting `attention_mask` tensor will have shape `(batch_size, seq_len - 1)` and will indicate which tokens are actual tokens (True) and which are padding (False).\n",
    "\n",
    "---\n",
    "\n",
    "6. **Return the Processed Batch**\n",
    "\n",
    "```python\n",
    "return input_ids, target_ids, attention_mask\n",
    "```\n",
    "\n",
    "- **Return Values**:\n",
    "  - **`input_ids`**: The input tokens (all but the last token) of shape `(batch_size, seq_len - 1)`.\n",
    "  - **`target_ids`**: The target tokens (all but the first token) of shape `(batch_size, seq_len - 1)`.\n",
    "  - **`attention_mask`**: A mask of shape `(batch_size, seq_len - 1)` that indicates which tokens are real and which are padding.\n",
    "\n",
    "These values are now ready for input to a model that performs next-token prediction. The model will use `input_ids` to predict the next token, and the loss can be computed by comparing the predictions with `target_ids`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can print an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_shakespeare()\n",
    "tokenizer = CharTokenizer(text)\n",
    "max_seq_len = 100\n",
    "\n",
    "dataset = TextDataset(text, max_seq_len)\n",
    "collate_fn = CollateFn(tokenizer, max_seq_len)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([46, 47, 41, 46,  1, 57, 46, 39, 50, 50,  1, 40, 56, 43, 39, 49,  1, 46,\n",
       "         47, 57,  1, 52, 43, 41, 49,  1, 53, 56,  1, 46, 39, 64, 39, 56, 42,  1,\n",
       "         51, 47, 52, 43,  6,  0, 35, 46, 43, 52, 43,  5, 43, 56,  1, 61, 43,  1,\n",
       "         41, 53, 51, 43,  1, 58, 53,  1, 53, 59, 56,  1, 39, 41, 41, 53, 59, 52,\n",
       "         58,  8,  0,  0, 24, 47, 43, 59, 58, 43, 52, 39, 52, 58, 10,  0, 31, 47,\n",
       "         56,  6,  1, 21,  1, 40, 43, 57, 43]),\n",
       " tensor([47, 41, 46,  1, 57, 46, 39, 50, 50,  1, 40, 56, 43, 39, 49,  1, 46, 47,\n",
       "         57,  1, 52, 43, 41, 49,  1, 53, 56,  1, 46, 39, 64, 39, 56, 42,  1, 51,\n",
       "         47, 52, 43,  6,  0, 35, 46, 43, 52, 43,  5, 43, 56,  1, 61, 43,  1, 41,\n",
       "         53, 51, 43,  1, 58, 53,  1, 53, 59, 56,  1, 39, 41, 41, 53, 59, 52, 58,\n",
       "          8,  0,  0, 24, 47, 43, 59, 58, 43, 52, 39, 52, 58, 10,  0, 31, 47, 56,\n",
       "          6,  1, 21,  1, 40, 43, 57, 43, 43]),\n",
       " tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True]),\n",
       " tensor([True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True, True, True, True, True, True, True, True, True, True,\n",
       "         True, True, True]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_idxs, target_idxs, input_attention_mask, target_attention_mask  = next(iter(dataloader))\n",
    "input_idxs[0], target_idxs[0], input_attention_mask[0], target_attention_mask[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, let's create a Pytorch Lightning data module, and also create a split of the dataset with a train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, text, tokenizer, max_seq_len, batch_size):\n",
    "        super().__init__()\n",
    "        self.text = text\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = TextDataset(self.text, self.max_seq_len)\n",
    "\n",
    "        train_size = int(0.9 * len(dataset))\n",
    "        val_size = len(dataset) - train_size\n",
    "        self.train_dataset, self.val_dataset = torch.utils.data.random_split(\n",
    "            dataset, [train_size, val_size]\n",
    "        )\n",
    "\n",
    "        self.collate_fn = CollateFn(self.tokenizer, self.max_seq_len)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, collate_fn=self.collate_fn)\n",
    "    \n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, collate_fn=self.collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using a Transformer model to predict the next token. The purpose of this class is not to learn how to code the Transformer, so it is provided for you in the `transformer.py` file, already prepared with the **causal mask** for next-token prediction as explained in class. This model is properly explained in the Deep Learning class of the Master. The rest of the Pytorch Lightning Module is explained after the code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShakespeareLightningModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        # Transformer params\n",
    "        vocab_size: int,\n",
    "        d_model: int,\n",
    "        nhead: int,\n",
    "        dim_feedforward: int,\n",
    "        dropout: float,\n",
    "        num_layers: int,\n",
    "        # Embedding params\n",
    "        padding_idx: int,\n",
    "        # Positional encoding params\n",
    "        max_len: int,\n",
    "        # Training params\n",
    "        optimizer_params: dict,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        # Transformer model\n",
    "        self.model = Transformer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            num_layers=num_layers,\n",
    "        )\n",
    "\n",
    "        # Embedding layers\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)\n",
    "        # self.pos_embedding = nn.Embedding(max_len, d_model)\n",
    "\n",
    "        # Classification head\n",
    "        self.fc = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "        # Training params\n",
    "        self.optimizer_params = optimizer_params\n",
    "\n",
    "    def forward(self, input_idxs, attention_mask):\n",
    "        input_embed = self.embedding(input_idxs)  # (batch_size, seq_len, d_model)\n",
    "\n",
    "        _, seq_len, d_model = input_embed.size()\n",
    "\n",
    "        # NOTE Option 1: Learnable positional embeddings\n",
    "        # positions = (\n",
    "        #     torch.arange(seq_len, device=self.device)\n",
    "        #     .unsqueeze(0)\n",
    "        #     .to(self.device)\n",
    "        # )  # (1, seq_len)\n",
    "        # pos_embeddings = self.pos_embedding(positions)  # (1, seq_len, d_model)\n",
    "\n",
    "        # NOTE Option 2: Sinusoidal positional embeddings\n",
    "        pos_embeddings = (\n",
    "            SinusoidalPositionalEncoding\n",
    "            .get_positional_encoding(seq_len, d_model)\n",
    "            .to(self.device)\n",
    "            .unsqueeze(0)\n",
    "        )  # (1, seq_len, d_model)\n",
    "\n",
    "        input_embed = input_embed + pos_embeddings\n",
    "        output = self.model(input_embed, attention_mask)\n",
    "        return self.fc(output)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True, on_epoch=True, on_step=True)\n",
    "        return loss\n",
    "     \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        loss = self._step(batch)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True, on_epoch=True, on_step=False)\n",
    "        return loss\n",
    "    \n",
    "    def _step(self, batch):\n",
    "        input_idxs, target_idxs, input_attention_mask, target_attention_mask = batch  # (batch_size, seq_len - 1)\n",
    "        output = self(input_idxs, input_attention_mask)  # (batch_size, seq_len - 1, vocab_size)\n",
    "\n",
    "        B, L, V = output.size()\n",
    "        loss = F.cross_entropy(output.view(B * L, V), target_idxs.view(-1), reduction=\"none\")  # (B * L)\n",
    "\n",
    "        # Mask out the padding tokens\n",
    "        target_attention_mask = target_attention_mask.view(-1) / target_attention_mask.sum()  # (B * L)\n",
    "        loss = (loss * target_attention_mask).sum()\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.AdamW(self.model.parameters(), **self.optimizer_params)\n",
    "        return optimizer\n",
    "    \n",
    "    def configure_callbacks(self):\n",
    "        return super().configure_callbacks() + [\n",
    "            pl.callbacks.ModelCheckpoint(monitor=\"val_loss\"),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step-by-Step Explanation of the `ShakespeareLightningModel` Class\n",
    "\n",
    "This class defines a PyTorch Lightning model for a transformer-based architecture, likely for sequence modeling tasks (e.g., next-token prediction) with specific configurations for text generation. Below is a detailed breakdown of the class and its methods.\n",
    "\n",
    "---\n",
    "\n",
    "### 1. **Class Definition: `ShakespeareLightningModel`**\n",
    "\n",
    "```python\n",
    "class ShakespeareLightningModel(pl.LightningModule):\n",
    "```\n",
    "\n",
    "- **`ShakespeareLightningModel`**: This class inherits from `pl.LightningModule`, which is a base class in PyTorch Lightning designed to simplify training, validation, and testing.\n",
    "- **Purpose**: It models a sequence-to-sequence task using a Transformer architecture with added capabilities for token classification (next-token prediction, etc.).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. **Constructor: `__init__`**\n",
    "\n",
    "```python\n",
    "def __init__(\n",
    "    self,\n",
    "    # Transformer params\n",
    "    vocab_size: int,\n",
    "    d_model: int,\n",
    "    nhead: int,\n",
    "    dim_feedforward: int,\n",
    "    dropout: float,\n",
    "    num_layers: int,\n",
    "    # Embedding params\n",
    "    padding_idx: int,\n",
    "    # Positional encoding params\n",
    "    max_len: int,\n",
    "    # Training params\n",
    "    optimizer_params: dict,\n",
    "):\n",
    "```\n",
    "\n",
    "- **Purpose**: Initializes the model by setting hyperparameters and defining various components of the transformer architecture.\n",
    "  \n",
    "#### Parameters:\n",
    "\n",
    "- **`vocab_size`**: The size of the vocabulary. This defines the number of unique tokens in the input text.\n",
    "- **`d_model`**: The dimensionality of the model's hidden state (the size of the embeddings and attention layers).\n",
    "- **`nhead`**: The number of attention heads in the multi-head self-attention mechanism.\n",
    "- **`dim_feedforward`**: The size of the hidden feedforward layer inside the transformer blocks.\n",
    "- **`dropout`**: Dropout rate used for regularization.\n",
    "- **`num_layers`**: Number of transformer layers in the model.\n",
    "- **`padding_idx`**: Index used for padding in the input sequences. Padding tokens are ignored in loss computation and attention.\n",
    "- **`max_len`**: Maximum sequence length for positional encoding.\n",
    "- **`optimizer_params`**: A dictionary of parameters that define how the optimizer will behave during training (e.g., learning rate, weight decay, etc.).\n",
    "\n",
    "#### Inside the constructor:\n",
    "\n",
    "```python\n",
    "super().__init__()\n",
    "self.save_hyperparameters()\n",
    "```\n",
    "- **`super().__init__()`**: Calls the constructor of the parent class (`pl.LightningModule`).\n",
    "- **`self.save_hyperparameters()`**: Saves the hyperparameters defined in the constructor, which is a convenience method provided by PyTorch Lightning. It stores the arguments in the model, making it easy to save/load the model configuration.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. **Defining Model Components**\n",
    "\n",
    "#### Transformer Model:\n",
    "\n",
    "```python\n",
    "self.model = Transformer(\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "```\n",
    "\n",
    "- **`self.model`**: A decoder-only Transformer model object that consists of multiple layers of multi-head **causal** self-attention and feed-forward networks. It's initialized with the parameters provided (e.g., model dimensions, number of layers).\n",
    "  \n",
    "#### Embedding Layer:\n",
    "\n",
    "```python\n",
    "self.embedding = nn.Embedding(vocab_size, d_model, padding_idx=padding_idx)\n",
    "```\n",
    "\n",
    "- **`self.embedding`**: An embedding layer that converts input tokens (represented by their indices) into dense vectors of size `d_model`.\n",
    "  - **`vocab_size`**: The number of tokens in the vocabulary.\n",
    "  - **`d_model`**: The dimensionality of the embeddings.\n",
    "  - **`padding_idx`**: Specifies the index that corresponds to the padding token, which will be ignored during training.\n",
    "\n",
    "#### Classification Head (Output Layer):\n",
    "\n",
    "```python\n",
    "self.fc = nn.Linear(d_model, vocab_size)\n",
    "```\n",
    "\n",
    "- **`self.fc`**: A fully connected (linear) layer that maps the final output representation from the transformer (`d_model`-dimensional) to the size of the vocabulary (`vocab_size`). This layer is used for classification (i.e., predicting the next token in a sequence).\n",
    "\n",
    "---\n",
    "\n",
    "### 4. **Forward Pass: `forward`**\n",
    "\n",
    "```python\n",
    "def forward(self, input_idxs, attention_mask):\n",
    "```\n",
    "\n",
    "- **`forward(self, input_idxs, attention_mask)`**: This method defines how the data flows through the model. It takes two inputs:\n",
    "  - **`input_idxs`**: The token indices of the input sequence, typically shaped `(batch_size, seq_len)`.\n",
    "  - **`attention_mask`**: A mask tensor indicating which tokens are padding (`0`) and which are real tokens (`1`).\n",
    "\n",
    "#### Input Embedding:\n",
    "\n",
    "```python\n",
    "input_embed = self.embedding(input_idxs)  # (batch_size, seq_len, d_model)\n",
    "```\n",
    "\n",
    "- **`input_embed`**: The input token indices (`input_idxs`) are passed through the embedding layer, resulting in dense representations of the input tokens with shape `(batch_size, seq_len, d_model)`.\n",
    "\n",
    "#### Positional Encoding (Sinusoidal):\n",
    "\n",
    "```python\n",
    "pos_embeddings = (\n",
    "    SinusoidalPositionalEncoding\n",
    "    .get_positional_encoding(seq_len, d_model)\n",
    "    .to(self.device)\n",
    "    .unsqueeze(0)\n",
    ")  # (1, seq_len, d_model)\n",
    "```\n",
    "\n",
    "- **`pos_embeddings`**: The model uses **sinusoidal positional encodings** to provide information about the relative or absolute position of tokens in the sequence. This encoding is added to the token embeddings to preserve order information.\n",
    "  - **`get_positional_encoding(seq_len, d_model)`**: This function generates sinusoidal positional encodings of size `(seq_len, d_model)`.\n",
    "  - **`.unsqueeze(0)`**: Adds a batch dimension, so the shape becomes `(1, seq_len, d_model)` (since the positional encoding is shared across the batch).\n",
    "  \n",
    "#### Combining Input Embeddings and Positional Encodings:\n",
    "\n",
    "```python\n",
    "input_embed = input_embed + pos_embeddings\n",
    "```\n",
    "\n",
    "- **Combining**: The learned token embeddings (`input_embed`) and the positional encodings (`pos_embeddings`) are summed element-wise. This gives the model information about both the content of the tokens and their positions in the sequence.\n",
    "\n",
    "#### Passing Through Transformer:\n",
    "\n",
    "```python\n",
    "output = self.model(input_embed, attention_mask)\n",
    "```\n",
    "\n",
    "- **`output`**: The transformed input embeddings (with positional encodings) are passed through the Transformer model. The `attention_mask` ensures that padding tokens are ignored during attention calculation.\n",
    "\n",
    "#### Final Output Layer:\n",
    "\n",
    "```python\n",
    "return self.fc(output)\n",
    "```\n",
    "\n",
    "- **`self.fc(output)`**: The transformer output (`output`) is passed through the final linear layer (`self.fc`) to get the prediction for each token in the sequence. The output shape is `(batch_size, seq_len, vocab_size)`, where each token in the sequence has a probability distribution over the vocabulary.\n",
    "\n",
    "---\n",
    "\n",
    "### 5. **Training Step: `_step`**\n",
    "\n",
    "```python\n",
    "def _step(self, batch):\n",
    "    input_idxs, target_idxs, input_attention_mask, target_attention_mask = batch  # (batch_size, seq_len - 1)\n",
    "    output = self(input_idxs, input_attention_mask)  # (batch_size, seq_len - 1, vocab_size)\n",
    "\n",
    "    B, L, V = output.size()\n",
    "    loss = F.cross_entropy(output.view(B * L, V), target_idxs.view(-1), reduction=\"none\")  # (B * L)\n",
    "\n",
    "    # Mask out the padding tokens\n",
    "    target_attention_mask = target_attention_mask.view(-1) / target_attention_mask.sum()  # (B * L)\n",
    "    loss = (loss * target_attention_mask).sum()\n",
    "\n",
    "    return loss\n",
    "```\n",
    "\n",
    "- **Purpose**: This method defines a single step of training: from the forward pass to loss computation.\n",
    "  \n",
    "#### Extract Batch Elements:\n",
    "\n",
    "```python\n",
    "input_idxs, target_idxs, attention_mask = batch\n",
    "```\n",
    "\n",
    "#### Forward Pass:\n",
    "\n",
    "```python\n",
    "output = self(input_idxs, input_attention_mask)  # (batch_size, seq_len - 1, vocab_size)\n",
    "```\n",
    "\n",
    "- **`output`**: The model's output after the forward pass is a tensor of shape `(batch_size, seq_len - 1, vocab_size)`, where each token in the sequence has a probability distribution over the vocabulary.\n",
    "\n",
    "#### Loss Computation:\n",
    "\n",
    "```python\n",
    "B, L, V = output.size()\n",
    "loss = F.cross_entropy(output.view(B * L, V), target_idxs.view(-1), reduction=\"none\")  # (B * L)\n",
    "\n",
    "# Mask out the padding tokens\n",
    "target_attention_mask = target_attention_mask.view(-1) / target_attention_mask.sum()  # (B * L)\n",
    "loss = (loss * target_attention_mask).sum()\n",
    "```\n",
    "\n",
    "- **`F.cross_entropy`**: The Cross Entropy Loss is computed between the predicted output and the target indices.\n",
    "  - The output is reshaped to `(B * L, V)` for proper loss computation (flattening the batch and sequence length dimensions).\n",
    "  - `target_idxs.view(-1)` flattens the target indices to match the reshaped output.\n",
    "  - `reduction=\"none\"` means that the loss functions is not reduced across elements, as we are used to. Typically, a mean over all elements in the batch (and sequence) are calculated. However, we include padding in these sequences, and we do not need to predict that. Thus, we prevent this basic reduction, and do it manually ourselves.\n",
    "  - **`target_attention_mask = target_attention_mask.view(-1) / target_attention_mask.sum()`**: This specifies the coefficients we use to compute the mean across elements in the batch and the sequences. `target_attention_mask.sum()` is the total amount of non-padding elements in the target ids. Thus, each non-padding element will be multiplied by the `1 / target_attention_mask.sum()` to compute the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = load_shakespeare()\n",
    "tokenizer = CharTokenizer(text)\n",
    "\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "padding_idx = tokenizer.get_padding_idx()\n",
    "\n",
    "d_model = 128\n",
    "nhead = 8\n",
    "dim_feedforward = 512\n",
    "dropout = 0.1\n",
    "num_layers = 8\n",
    "\n",
    "max_seq_len = 100\n",
    "\n",
    "batch_size = 64\n",
    "optimizer_params = dict(lr=3e-4, weight_decay=1e-2)\n",
    "max_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = ShakespeareDataModule(text, tokenizer, max_seq_len, batch_size)\n",
    "data_module.setup()\n",
    "\n",
    "model = ShakespeareLightningModel(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=d_model,\n",
    "    nhead=nhead,\n",
    "    dim_feedforward=dim_feedforward,\n",
    "    dropout=dropout,\n",
    "    num_layers=num_layers,\n",
    "    padding_idx=padding_idx,\n",
    "    max_len=max_seq_len,\n",
    "    optimizer_params=optimizer_params,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pablo/.micromamba/envs/mdl_gen/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n"
     ]
    }
   ],
   "source": [
    "trainer = pl.Trainer(\n",
    "    max_epochs=max_epochs,\n",
    "    accelerator=\"gpu\",\n",
    "    devices=[1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following callbacks returned in `LightningModule.configure_callbacks` will override existing callbacks passed to Trainer: ModelCheckpoint\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name      | Type        | Params\n",
      "------------------------------------------\n",
      "0 | model     | Transformer | 1.6 M \n",
      "1 | embedding | Embedding   | 8.4 K \n",
      "2 | fc        | Linear      | 8.5 K \n",
      "------------------------------------------\n",
      "1.6 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.6 M     Total params\n",
      "6.413     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/mdl_gen/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/mdl_gen/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=47` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 157/157 [00:04<00:00, 37.21it/s, v_num=28, train_loss_step=1.340, val_loss=1.420, train_loss_epoch=1.350]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 157/157 [00:04<00:00, 36.57it/s, v_num=28, train_loss_step=1.340, val_loss=1.420, train_loss_epoch=1.350]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(model, data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now generate some text! We implement two of the most simple decoding strategies seen in class:\n",
    "1. **Greedy**: always choosing the most likely token.\n",
    "2. **Sampling**: random sampling from the probability distribution as is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoding(logits):\n",
    "    # logits: (batch_size, vocab_size)\n",
    "    return logits.argmax(dim=-1)\n",
    "\n",
    "def sampling_decoding(logits):\n",
    "    # logits: (batch_size, vocab_size)\n",
    "    return torch.multinomial(F.softmax(logits, dim=-1), num_samples=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_text(\n",
    "    decoding_fn,\n",
    "    text,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    num_completions=3,\n",
    "    max_len=100,\n",
    "):\n",
    "    device = next(model.parameters()).device\n",
    "\n",
    "    for _ in range(num_completions):\n",
    "        encoded_text = tokenizer.encode(text)[-max_len:]  # NOTE: Truncate the text! The model was\n",
    "                                                          # trained with a maximum length, and will \n",
    "                                                          # break down if the input is longer than that.\n",
    "        encoded_text = torch.tensor(encoded_text).unsqueeze(0).to(device)\n",
    "        attention_mask = torch.tensor([[True] * len(encoded_text[0])]).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            output = model(encoded_text, attention_mask)  # (1, seq_len, vocab_size)\n",
    "            output = output[:, -1, :]  # (1, vocab_size)\n",
    "\n",
    "            next_tokens = decoding_fn(output)  # (1, 1)\n",
    "\n",
    "            predicted_char = tokenizer.idx_to_char[next_tokens.item()]\n",
    "\n",
    "        text += predicted_char\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can load the model from disk if you have trained on a previous run of the notebook:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CKPT_PATH = \"lightning_logs/version_26/checkpoints/epoch=49-step=7850.ckpt\"\n",
    "# model = ShakespeareLightningModel.load_from_checkpoint(CKPT_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ShakespeareLightningModel(\n",
       "  (model): Transformer(\n",
       "    (layers): ModuleList(\n",
       "      (0-7): 8 x CausalTransformerLayer(\n",
       "        (attn): MultiheadAttention(\n",
       "          (q_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (k_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (v_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "          (out_proj): Linear(in_features=128, out_features=128, bias=True)\n",
       "        )\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "        (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "        (ff): Sequential(\n",
       "          (0): Linear(in_features=128, out_features=512, bias=True)\n",
       "          (1): GELU(approximate='none')\n",
       "          (2): Linear(in_features=512, out_features=128, bias=True)\n",
       "        )\n",
       "        (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (embedding): Embedding(66, 128, padding_idx=65)\n",
       "  (fc): Linear(in_features=128, out_features=66, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.to(\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, that is the strange of the\n",
      "should be so contented the state.\n",
      "\n",
      "POLIXENES:\n",
      "What is the son?\n",
      "\n",
      "\n",
      "KING EDWARD IV:\n",
      "The master of the comple of his household be his honour,\n",
      "That thou hast a breathed the cannot be contented\n",
      "That thou canst the seal that they shall be strange\n",
      "That with the strange of the contrainted of the date.\n",
      "\n",
      "\n",
      "PROSPERO:\n",
      "The provost of SaintAble and love to the seath.\n",
      "\n",
      "\n",
      "POMPEY:\n",
      "What she had beeen the stan of the strange?\n",
      "\n",
      "\n",
      "POMPSPEY:\n",
      "The good love another than the senators of the day.\n",
      "\n",
      "\n",
      "PETRUCHIO:\n",
      "Not I willl not so for the stand and long the thought of the\n",
      "that thou shalt be the dead of this day wont,\n",
      "That with the shall be the senator's day,\n",
      "That thou didst be the strange of the comple\n",
      "To the coward of the consins and like the complainty.\n",
      "\n",
      "\n",
      "KING RICHARD III:\n",
      "The may soul that well hath the doth of the death.\n",
      "\n",
      "\n",
      "QUEEN ELIZABETH:\n",
      "The great that would be the stimph of the contrate\n",
      "To the fire of this wof the power of the death.\n",
      "\n",
      "\n",
      "POLIXENES:\n",
      "What is the sworn of Marcius?\n",
      "\n",
      "\n",
      "SICINIUS:\n",
      "The prov\n"
     ]
    }
   ],
   "source": [
    "text = \"To be or not to be, that is the \"\n",
    "completed_text = complete_text(greedy_decoding, text, tokenizer, model, num_completions=1000, max_len=max_seq_len)\n",
    "print(completed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To be or not to be, that is the friar!\n",
      "\n",
      "ANTONIO:\n",
      "Come, I kmardled at thXpeal,\n",
      "That all us, and trickely more than sherm  with\n",
      "scorn!\n",
      "The thown some speeaking of r and bids vaultlat, no\n",
      "Forth that thou are:\n",
      "Calll to breothe in ownderous; that I did\n",
      "Such rider I in the isguing,\n",
      "And give the firor lionghs in cait, death incennt.\n",
      "In soorlaly and by iss reharly 'O mine!'\n",
      "\n",
      "Shepherd:\n",
      "In a mine againstar off wons scape undertake\n",
      "The appehing into againsst of trorclay of his son!\n",
      "But loves their nest:\n",
      "Outward boldling away, sirve he disg man;\n",
      "Nothing to unwits thy brader panI thanke chat\n",
      "it iss toward Lay, all traTh thou dast!\n",
      "Good to those hath canon, cannot bid:\n",
      "TowDo fought of Warwick, that was thou not the stat\n",
      "On of not to son wit, wan with o'clock ond\n",
      "Thard hath that olst thought thou thinkest an'st edColl'd\n",
      "t by dost she bliftt winkels within thy angree.\n",
      "Somp my cunglioc and us ous it wordon hasband!\n",
      "What is mind own your freshelf deseated?\n",
      "What vizes saw is limberard, sweeet with O,\n",
      "Gro unfrience; but shike adoubt how\n"
     ]
    }
   ],
   "source": [
    "text = \"To be or not to be, that is the \"\n",
    "completed_text = complete_text(sampling_decoding, text, tokenizer, model, num_completions=1000, max_len=max_seq_len)\n",
    "print(completed_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
