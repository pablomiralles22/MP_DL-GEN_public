{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 3 - Exercise notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CQ1. (☆☆☆) Complete the following GAN code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/mdl_gen/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image\n",
    "from datasets import load_dataset\n",
    "\n",
    "import torch.multiprocessing as mp\n",
    "mp.set_start_method('fork', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CollateFn:\n",
    "    def __init__(self, transform):\n",
    "        self.transform = transform\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        images = torch.stack([self.transform(item.pop(\"image\")) for item in batch])\n",
    "        return images, batch\n",
    "    \n",
    "class CelebADataModule(pl.LightningDataModule):\n",
    "    def __init__(self, train_batch_size, val_batch_size, image_size):\n",
    "        super().__init__()\n",
    "        self.train_batch_size = train_batch_size\n",
    "        self.val_batch_size = val_batch_size\n",
    "        self.image_size = image_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        dataset = load_dataset(\"tpremoli/CelebA-attrs\")\n",
    "        self.train_dataset = dataset[\"train\"]\n",
    "        self.val_dataset = dataset[\"validation\"]\n",
    "\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.CenterCrop(self.image_size),\n",
    "            transforms.RandomHorizontalFlip(),  # A bit of augmentation\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.train_collate_fn = CollateFn(train_transform)\n",
    "\n",
    "        val_transform = transforms.Compose([\n",
    "            transforms.Resize(self.image_size),\n",
    "            transforms.CenterCrop(self.image_size),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.val_collate_fn = CollateFn(val_transform)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.train_batch_size,\n",
    "            shuffle=True,\n",
    "            collate_fn=self.train_collate_fn,\n",
    "            num_workers=16,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.val_batch_size,\n",
    "            shuffle=False,\n",
    "            collate_fn=self.val_collate_fn,\n",
    "            num_workers=16,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generator Network\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=128):\n",
    "        super(Generator, self).__init__()\n",
    "        self.fc1 = nn.Linear(z_dim, 1024 * 2 * 2)\n",
    "        self.nn = nn.Sequential(\n",
    "            *self.__get_decoder_block(1024, 512),  # 1024x2x2 -> 512x4x4\n",
    "            *self.__get_decoder_block(512, 256),  # 512x4x4 -> 256x8x8\n",
    "            *self.__get_decoder_block(256, 128),  # 256x8x8 -> 128x16x16\n",
    "            *self.__get_decoder_block(128, 64),  # 128x16x16 -> 64x32x32\n",
    "            *self.__get_decoder_block(64, 32),  # 64x32x32 -> 32x64x64\n",
    "            nn.Conv2d(32, 3, kernel_size=3, stride=1, padding=1),  # 16x64x64 -> 3x64x64\n",
    "            nn.Sigmoid(),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        x = self.fc1(z).view(-1, 1024, 2, 2)\n",
    "        return self.nn(x)\n",
    "    \n",
    "    def __get_decoder_block(self, input_channels, output_channels):\n",
    "        return [\n",
    "            nn.ConvTranspose2d(input_channels, output_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        ]\n",
    "\n",
    "# Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            *self.__get_encoder_block(3, 64),  # 3x64x64 -> 64x32x32\n",
    "            *self.__get_encoder_block(64, 128),  # 64x32x32 -> 128x16x16\n",
    "            *self.__get_encoder_block(128, 256),  # 128x16x16 -> 256x8x8\n",
    "            *self.__get_encoder_block(256, 512),  # 256x8x8 -> 512x4x4\n",
    "            *self.__get_encoder_block(512, 1024),  # 512x4x4 -> 1024x2x2\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(1024 * 2 * 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "    def __get_encoder_block(self, input_channels, output_channels):\n",
    "        return [\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModule(pl.LightningModule):\n",
    "    def __init__(self, opt_g_params, opt_d_params, z_dim=128):\n",
    "        super(GANModule, self).__init__()\n",
    "        self.generator = Generator(z_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "        self.z_dim = z_dim\n",
    "        self.opt_g_params = opt_g_params\n",
    "        self.opt_d_params = opt_d_params\n",
    "\n",
    "        # NOTE: Disable automatic optimization to manually control the training loop\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step for the GAN module. We need to perform the optimization manually,\n",
    "        so we disable the automatic optimization and implement the training loop ourselves.\n",
    "        Each training loop consists of two steps:\n",
    "            1. Train the discriminator\n",
    "            2. Train the generator\n",
    "        \"\"\"\n",
    "        # Get optimizers\n",
    "        opt_D, opt_G = self.optimizers()\n",
    "\n",
    "        # ================================================== #\n",
    "        # ================== Prepare data ================== #\n",
    "        # ================================================== #\n",
    "        real_images, _ = batch  # [B, 3, 64, 64]\n",
    "        z = torch.randn(real_images.size(0), self.z_dim).to(self.device)  # [B, z_dim]\n",
    "        fake_images = self.generator(z)  # [B, 3, 64, 64]\n",
    "\n",
    "        # ========================================================= #\n",
    "        # ================== Train Discriminator ================== #\n",
    "        # ========================================================= #\n",
    "        opt_D.zero_grad()\n",
    "\n",
    "        D_real = self.discriminator(real_images)\n",
    "        D_fake = self.discriminator(fake_images.detach())  # NOTE: detach the fake_images tensor to avoid computing gradients for the generator\n",
    "        loss_D = (\n",
    "            F.binary_cross_entropy_with_logits(D_real, torch.ones_like(D_real)) +\n",
    "            F.binary_cross_entropy_with_logits(D_fake, torch.zeros_like(D_fake))\n",
    "        )\n",
    "\n",
    "        self.manual_backward(loss_D)\n",
    "        opt_D.step()\n",
    "\n",
    "        # ===================================================== #\n",
    "        # ================== Train Generator ================== #\n",
    "        # ===================================================== #\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        D_fake = self.discriminator(fake_images)\n",
    "        # loss_G = -F.binary_cross_entropy_with_logits(D_fake, torch.zeros_like(D_fake))  # NOTE: original GAN loss\n",
    "        loss_G = F.binary_cross_entropy_with_logits(D_fake, torch.ones_like(D_fake))  # NOTE: non-saturating GAN loss\n",
    "\n",
    "        self.manual_backward(loss_G)\n",
    "        opt_G.step()\n",
    "\n",
    "        # ================================================= #\n",
    "        # ================== Log Results ================== #\n",
    "        # ================================================= #\n",
    "        self.log(\"loss_D\", loss_D, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        self.log(\"loss_G\", loss_G, on_step=True, on_epoch=False, prog_bar=True)\n",
    "\n",
    "        return {\"loss_D\": loss_D, \"loss_G\": loss_G}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_D = optim.AdamW(self.discriminator.parameters(), **self.opt_d_params)\n",
    "        opt_G = optim.AdamW(self.generator.parameters(), **self.opt_g_params)\n",
    "        return [opt_D, opt_G], []\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Generate example images at the end of each epoch\n",
    "        \"\"\"\n",
    "        os.makedirs(\"output/exercise1\", exist_ok=True)\n",
    "        file_path = f\"output/exercise1/generator_epoch_{self.current_epoch}.png\"\n",
    "        z = torch.randn(64, self.z_dim).to(self.device)\n",
    "        generated_images = self.generator(z)\n",
    "        save_image(generated_images, file_path, nrow=8, normalize=True)\n",
    "        return super().on_train_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_BATCH_SIZE = 64\n",
    "VAL_BATCH_SIZE = 64\n",
    "IMAGE_SIZE = 64\n",
    "\n",
    "ZDIM = 128\n",
    "\n",
    "OPT_G_PARAMS = {\n",
    "    \"lr\": 1.5e-4,\n",
    "    \"betas\": (0.5, 0.999),\n",
    "}\n",
    "OPT_D_PARAMS = {\n",
    "    \"lr\": 1e-4,\n",
    "    \"betas\": (0.5, 0.999),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pablo/.micromamba/envs/mdl_gen/lib/python3.11/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "data_module = CelebADataModule(TRAIN_BATCH_SIZE, VAL_BATCH_SIZE, IMAGE_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "/home/pablo/.micromamba/envs/mdl_gen/lib/python3.11/site-packages/pytorch_lightning/trainer/connectors/logger_connector/logger_connector.py:67: Starting from v1.9.0, `tensorboardX` has been removed as a dependency of the `pytorch_lightning` package, due to potential conflicts with other packages in the ML ecosystem. For this reason, `logger=True` will use `CSVLogger` as the default logger, unless the `tensorboard` or `tensorboardX` packages are found. Please `pip install lightning[extra]` or one of them to enable TensorBoard support by default\n",
      "/home/pablo/.micromamba/envs/mdl_gen/lib/python3.11/site-packages/pytorch_lightning/trainer/configuration_validator.py:72: You passed in a `val_dataloader` but have no `validation_step`. Skipping val loop.\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 3090') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "Missing logger folder: /home/pablo/classes/MP_DL-GEN/Lecture 3 - Introduction to Generative Adversarial Networks/lightning_logs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 6.8 M \n",
      "1 | discriminator | Discriminator | 6.3 M \n",
      "------------------------------------------------\n",
      "13.1 M    Trainable params\n",
      "0         Non-trainable params\n",
      "13.1 M    Total params\n",
      "52.385    Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2544/2544 [01:51<00:00, 22.88it/s, v_num=0, loss_D=0.00542, loss_G=6.440]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 2544/2544 [01:51<00:00, 22.76it/s, v_num=0, loss_D=0.00542, loss_G=6.440]\n"
     ]
    }
   ],
   "source": [
    "model = GANModule(OPT_G_PARAMS, OPT_D_PARAMS, ZDIM)\n",
    "trainer = pl.Trainer(max_epochs=20, accelerator=\"gpu\", devices=[0])\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CQ2. (☆☆☆☆☆) Modify the GAN code to make it a WGAN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was already done in the lecture notebook!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CQ3. (☆☆☆☆) Modify the GAN code to make it a Conditional GAN.\n",
    "You may use any dataset you want, as long as it has some attribute to condition the output! We use CIFAR-10 in the solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we hide the answer because this is very similar to the assignment :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CQ4. (☆☆☆) Modify the GAN code by adding the Mode-Seeking loss from the slides.\n",
    "In this exercise, we will change the GAN code to add the Mode-Seeking loss from the slides. Please complete the spaces in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with synthetic data: we take the corners of a square and generate 2D points from a gaussian distribution around them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 239,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "CENTERS = [ [1.0, 1.0], [1.0, -1.0], [-1.0, 1.0], [-1.0, -1.0] ]\n",
    "STDS = [0.05, 0.05, 0.05, 0.05]\n",
    "\n",
    "def generate_gaussian_data(n_samples_per_mode=500, centers=CENTERS, stds=STDS):\n",
    "    data = []\n",
    "    for center, std in zip(centers, stds):\n",
    "        data.append(torch.randn(n_samples_per_mode, 2) * std + torch.tensor(center))\n",
    "    return torch.cat(data, dim=0)\n",
    "\n",
    "class SyntheticDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, n_samples_per_mode=500):\n",
    "        self.data = generate_gaussian_data(n_samples_per_mode)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]\n",
    "    \n",
    "class SyntheticDataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = './data', batch_size: int = 64, n_samples_per_mode=500):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "        self.n_samples_per_mode = n_samples_per_mode\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = SyntheticDataset(self.n_samples_per_mode)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "# plot the ground truth data\n",
    "data = generate_gaussian_data(1_000)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.scatter(data[:, 0], data[:, 1], s=10)\n",
    "for center in CENTERS:\n",
    "    plt.scatter(center[0], center[1], s=100, marker=\"x\", color=\"red\")\n",
    "plt.xlim(-4, 4)\n",
    "plt.ylim(-4, 4)\n",
    "plt.axis('off')\n",
    "plt.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, points are clustered around four points. These are different modes of the distribution. A GAN might end up collapsing on a few of them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build the generator and discriminator. This is already done for you!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim=4):\n",
    "        super().__init__()\n",
    "        self.ff_mlp = nn.Sequential(\n",
    "            nn.Linear(z_dim, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(128, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.ff_mlp(z)\n",
    "    \n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.ff_mlp = nn.Sequential(\n",
    "            nn.Linear(2, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.LeakyReLU(negative_slope=0.2),\n",
    "            nn.Linear(128, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.ff_mlp(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get to the GAN training module. Please complete the `__mode_seeking_loss` loss function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModule(pl.LightningModule):\n",
    "    def __init__(self, opt_g_params, opt_d_params, z_dim=4, mode_seeking_loss_weight=0.1):\n",
    "        super(GANModule, self).__init__()\n",
    "        self.generator = Generator(z_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "        self.z_dim = z_dim\n",
    "        self.opt_g_params = opt_g_params\n",
    "        self.opt_d_params = opt_d_params\n",
    "        self.mode_seeking_loss_weight = mode_seeking_loss_weight\n",
    "\n",
    "        # NOTE: Disable automatic optimization to manually control the training loop\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step for the GAN module. We need to perform the optimization manually,\n",
    "        so we disable the automatic optimization and implement the training loop ourselves.\n",
    "        Each training loop consists of two steps:\n",
    "            1. Train the discriminator\n",
    "            2. Train the generator\n",
    "        \"\"\"\n",
    "        # Get optimizers\n",
    "        opt_D, opt_G = self.optimizers()\n",
    "\n",
    "        # ================================================== #\n",
    "        # ================== Prepare data ================== #\n",
    "        # ================================================== #\n",
    "        real_samples = batch  # [B, 2]\n",
    "        z = self.__generate_noise(real_samples.size(0))  # [B, z_dim]\n",
    "        fake_samples = self.generator(z)  # [B, 2]\n",
    "\n",
    "        # ========================================================= #\n",
    "        # ================== Train Discriminator ================== #\n",
    "        # ========================================================= #\n",
    "        opt_D.zero_grad()\n",
    "\n",
    "        D_real = self.discriminator(real_samples)\n",
    "        D_fake = self.discriminator(fake_samples.detach())  # NOTE: detach the fake_samples tensor to avoid computing gradients for the generator\n",
    "        loss_D = (\n",
    "            F.binary_cross_entropy_with_logits(D_real, torch.ones_like(D_real)) +\n",
    "            F.binary_cross_entropy_with_logits(D_fake, torch.zeros_like(D_fake))\n",
    "        )\n",
    "\n",
    "        self.manual_backward(loss_D)\n",
    "        opt_D.step()\n",
    "\n",
    "        # ===================================================== #\n",
    "        # ================== Train Generator ================== #\n",
    "        # ===================================================== #\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        D_fake = self.discriminator(fake_samples)\n",
    "        # loss_G = -F.binary_cross_entropy_with_logits(D_fake, torch.zeros_like(D_fake))  # NOTE: original GAN loss\n",
    "        loss_G = F.binary_cross_entropy_with_logits(D_fake, torch.ones_like(D_fake))  # NOTE: non-saturating GAN loss\n",
    "        mode_seeking_loss = self.__mode_seeking_loss(z, fake_samples)\n",
    "\n",
    "        self.manual_backward(loss_G - self.mode_seeking_loss_weight * mode_seeking_loss)\n",
    "        opt_G.step()\n",
    "\n",
    "        # ================================================= #\n",
    "        # ================== Log Results ================== #\n",
    "        # ================================================= #\n",
    "        self.log(\"loss_D\", loss_D, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        self.log(\"loss_G\", loss_G, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        self.log(\"mode_seeking_loss\", mode_seeking_loss, on_step=True, on_epoch=False, prog_bar=True)\n",
    "\n",
    "        return {\"loss_D\": loss_D, \"loss_G\": loss_G}\n",
    "    \n",
    "    def __generate_noise(self, n_samples):\n",
    "        z_1 = torch.randn(n_samples, self.z_dim // 2).to(self.device)  # [B, z_dim]\n",
    "        z_2 = torch.randint(0, 2, (n_samples, self.z_dim // 2)).float().to(self.device)  # [B, z_dim]\n",
    "        return torch.cat((z_1, z_2), dim=1)\n",
    "    \n",
    "    def __mode_seeking_loss(self, z, x):\n",
    "        # Compute the Euclidean distances between each pair of points\n",
    "        z, x = z.flatten(1), x.flatten(1)\n",
    "\n",
    "        z_norm_mat = torch.cdist(z, z, p=1)\n",
    "        x_norm_mat = torch.cdist(x, x, p=1)\n",
    "        \n",
    "        return torch.mean(x_norm_mat / (z_norm_mat + 1e-6))\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_D = optim.AdamW(self.discriminator.parameters(), **self.opt_d_params)\n",
    "        opt_G = optim.AdamW(self.generator.parameters(), **self.opt_g_params)\n",
    "        return [opt_D, opt_G], []\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Generate example samples at the end of each epoch\n",
    "        \"\"\"\n",
    "        os.makedirs(\"output/mode_seeking_loss\", exist_ok=True)\n",
    "        file_path = f\"output/mode_seeking_loss/generator_epoch_{self.current_epoch}.png\"\n",
    "        z = self.__generate_noise(1000)\n",
    "        generated_samples = self.generator(z).detach().cpu().numpy()\n",
    "        # plot 2D points in the range [-4, 4] x [-4, 4]\n",
    "        plt.figure(figsize=(5, 5))\n",
    "        plt.scatter(generated_samples[:, 0], generated_samples[:, 1], s=10)\n",
    "        for center in CENTERS:\n",
    "            plt.scatter(center[0], center[1], s=100, marker=\"x\", color=\"red\")\n",
    "        plt.xlim(-4, 4)\n",
    "        plt.ylim(-4, 4)\n",
    "        plt.axis('off')\n",
    "        plt.savefig(file_path)\n",
    "        plt.close()\n",
    "        return super().on_train_epoch_end()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can now test the models with two different weights for the mode seeking loss. For me $1.5$ gave a good result, but depending on your implementation you might need to change the value. You can compare with a value of $0.0$ and see how the training dynamics change. In the `output_mode_seeking_loss` you will find the distribution produced by the generator after each epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZDIM = 8\n",
    "OPT_G_PARAMS = { \"lr\": 1e-4, \"weight_decay\": 0.01 }\n",
    "OPT_D_PARAMS = { \"lr\": 1e-4, \"weight_decay\": 0.01 }\n",
    "BATCH_SIZE = 4\n",
    "# MODE_SEEKING_LOSS_WEIGHT = 1.5\n",
    "MODE_SEEKING_LOSS_WEIGHT = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = SyntheticDataModule(batch_size=BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 50.9 K\n",
      "1 | discriminator | Discriminator | 50.0 K\n",
      "------------------------------------------------\n",
      "100 K     Trainable params\n",
      "0         Non-trainable params\n",
      "100 K     Total params\n",
      "0.404     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 500/500 [00:12<00:00, 39.68it/s, v_num=154, loss_D=0.957, loss_G=0.843, mode_seeking_loss=0.171]  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=50` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49: 100%|██████████| 500/500 [00:12<00:00, 39.38it/s, v_num=154, loss_D=0.957, loss_G=0.843, mode_seeking_loss=0.171]\n"
     ]
    }
   ],
   "source": [
    "model = GANModule(OPT_G_PARAMS, OPT_D_PARAMS, ZDIM, MODE_SEEKING_LOSS_WEIGHT)\n",
    "trainer = pl.Trainer(max_epochs=50, accelerator=\"gpu\", devices=[0])\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some comments on the output (you can find it in `output/mode_seeking_loss*`):\n",
    "- It might look like mode-seeking works much better because it didn't drop any mode, but it is a bit random depending on the run.\n",
    "- Something that is more consistent is that it seems to degenerate less early on, converging earlier to a solution with some of the modes. But again, it really depends on the run. You can run multiple times and try to study the dynamics!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CQ5. (☆☆☆☆☆) Modify the GAN code so that instead of generating images, the generator colours some input grayscale image\n",
    "**NOTE**: This exercise is not guided, you will have to modify a good part of the code on your own.\n",
    "\n",
    "Transform the GAN architecture so instead of generating images, the generator colors an input grayscaled image. Use the CIFAR-10 as dataset. Some hints:\n",
    "\n",
    "- Use the Lab color space instead of RGB. The L channel is just the grayscale, and you just have to predict the \"ab\" channels. You will need to modify the data loading process to transform the images (using `skimage.color` for example).\n",
    "- The generator is not a \"decoder\" now, from a small vector to a large image. It is now a full autoencoder.\n",
    "- We recommend using a U-Net architecture. Basically, you take the output at each step of downsampling use it during upsampling. This way, in the center of the network you get the high level features, but do not lose the exact spatial features from early layers.\n",
    "\n",
    " ![u-net](https://viso.ai/wp-content/uploads/2024/04/unet-process.png)\n",
    "   \n",
    " The image and a more in-depth tutorial can be found [here](https://viso.ai/deep-learning/u-net-a-comprehensive-guide-to-its-architecture-and-applications/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "import os\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.nn import functional as F\n",
    "from torchvision import transforms\n",
    "from torchvision.datasets import CIFAR10\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.utils import save_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10DataModule(pl.LightningDataModule):\n",
    "    def __init__(self, data_dir: str = './data', batch_size: int = 64):\n",
    "        super().__init__()\n",
    "        self.data_dir = data_dir\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def setup(self, stage=None):\n",
    "        self.train_dataset = CIFAR10(self.data_dir, train=True, download=True, transform=transforms.ToTensor())\n",
    "        self.val_dataset = CIFAR10(self.data_dir, train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4)\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_dataset, batch_size=self.batch_size, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetGenerator(nn.Module):\n",
    "    \"\"\"\n",
    "    UNet Generator model for image coloring. Skip-connections are implemented as sums,\n",
    "    but you can change them to concatenation if you prefer.\n",
    "    \"\"\"\n",
    "    def __init__(self, latent_dim=256):\n",
    "        super(UNetGenerator, self).__init__()\n",
    "\n",
    "        # Encoder (Contracting path)\n",
    "        self.enc1 = self.conv_block(1, 64)  # 1 input channel for L channel\n",
    "        self.enc2 = self.conv_block(64, 128)\n",
    "        self.enc3 = self.conv_block(128, 256)\n",
    "        self.enc4 = self.conv_block(256, 512)\n",
    "        self.enc5 = self.conv_block(512, 1024)\n",
    "        \n",
    "        # Latent space\n",
    "        self.fc = nn.Linear(1024, latent_dim)\n",
    "        self.fc_expand = nn.Sequential(\n",
    "            nn.Linear(latent_dim, 1024),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "        # Decoder (Expanding path)\n",
    "        self.dec1 = self.deconv_block(1024, 512)\n",
    "        self.dec2 = self.deconv_block(512, 256)\n",
    "        self.dec3 = self.deconv_block(256, 128)\n",
    "        self.dec4 = self.deconv_block(128, 64)\n",
    "        self.dec5 = self.deconv_block(64, 2)  # 2 output channels for AB channels\n",
    "\n",
    "        self.out_conv = nn.Conv2d(2, 2, kernel_size=1, stride=1, padding=0)  # 1x1 convolution for color prediction\n",
    "    \n",
    "    def conv_block(self, in_channels, out_channels):\n",
    "        \"\"\" Helper function for creating convolutional blocks \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "\n",
    "    def deconv_block(self, in_channels, out_channels):\n",
    "        \"\"\" Helper function for creating deconvolutional (transposed conv) blocks \"\"\"\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        batch_size = x.size(0)\n",
    "\n",
    "        # Encoder forward pass\n",
    "        enc1_out = self.enc1(x)\n",
    "        enc2_out = self.enc2(enc1_out)\n",
    "        enc3_out = self.enc3(enc2_out)\n",
    "        enc4_out = self.enc4(enc3_out)\n",
    "        enc5_out = self.enc5(enc4_out)\n",
    "        \n",
    "        # Flatten and pass through the latent space layers\n",
    "        flattened = enc5_out.view(batch_size, -1)  # Flatten the output\n",
    "        z = self.fc(flattened)\n",
    "        \n",
    "        # Expand the latent space and reshape for decoder\n",
    "        z = self.fc_expand(z)\n",
    "        \n",
    "        # Decoder forward pass\n",
    "        dec1_out = self.dec1(z.view(batch_size, 1024, 1, 1))  # Reshape z for decoder\n",
    "        dec2_out = self.dec2(dec1_out + enc4_out)  # Skip connection from encoder\n",
    "        dec3_out = self.dec3(dec2_out + enc3_out)\n",
    "        dec4_out = self.dec4(dec3_out + enc2_out)\n",
    "        dec5_out = self.dec5(dec4_out + enc1_out)\n",
    "\n",
    "        # Final output\n",
    "        x_hat = self.out_conv(dec5_out)\n",
    "        \n",
    "        return x_hat, z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Discriminator Network\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.nn = nn.Sequential(\n",
    "            *self.__get_encoder_block(3, 32),   # 3x32x32 -> 32x16x16\n",
    "            *self.__get_encoder_block(32, 64),  # 32x16x16 -> 64x8x8\n",
    "            *self.__get_encoder_block(64, 128), # 64x8x8 -> 128x4x4\n",
    "            *self.__get_encoder_block(128, 256),# 128x4x4 -> 256x2x2\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(256 * 2 * 2, 1),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.nn(x)\n",
    "\n",
    "    def __get_encoder_block(self, input_channels, output_channels):\n",
    "        return [\n",
    "            nn.Conv2d(input_channels, output_channels, kernel_size=3, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(output_channels),\n",
    "            nn.GELU(),\n",
    "        ]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANModule(pl.LightningModule):\n",
    "    def __init__(self, opt_g_params, opt_d_params, z_dim=128):\n",
    "        super(GANModule, self).__init__()\n",
    "        self.generator = Generator(z_dim)\n",
    "        self.discriminator = Discriminator()\n",
    "        self.z_dim = z_dim\n",
    "        self.opt_g_params = opt_g_params\n",
    "        self.opt_d_params = opt_d_params\n",
    "\n",
    "        # NOTE: Disable automatic optimization to manually control the training loop\n",
    "        self.automatic_optimization = False\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.generator(z)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Training step for the GAN module. We need to perform the optimization manually,\n",
    "        so we disable the automatic optimization and implement the training loop ourselves.\n",
    "        Each training loop consists of two steps:\n",
    "            1. Train the discriminator\n",
    "            2. Train the generator\n",
    "        \"\"\"\n",
    "        # Get optimizers\n",
    "        opt_D, opt_G = self.optimizers()\n",
    "\n",
    "        # ================================================== #\n",
    "        # ================== Prepare data ================== #\n",
    "        # ================================================== #\n",
    "        real_images, _ = batch  # [B, 3, 64, 64]\n",
    "        z = torch.randn(real_images.size(0), self.z_dim).to(self.device)  # [B, z_dim]\n",
    "        fake_images = self.generator(z)  # [B, 3, 64, 64]\n",
    "\n",
    "        # ========================================================= #\n",
    "        # ================== Train Discriminator ================== #\n",
    "        # ========================================================= #\n",
    "        opt_D.zero_grad()\n",
    "\n",
    "        D_real = self.discriminator(real_images)\n",
    "        D_fake = self.discriminator(fake_images.detach())  # NOTE: detach the fake_images tensor to avoid computing gradients for the generator\n",
    "        loss_D = (\n",
    "            F.binary_cross_entropy_with_logits(D_real, torch.ones_like(D_real)) +\n",
    "            F.binary_cross_entropy_with_logits(D_fake, torch.zeros_like(D_fake))\n",
    "        )\n",
    "\n",
    "        self.manual_backward(loss_D)\n",
    "        opt_D.step()\n",
    "\n",
    "        # ===================================================== #\n",
    "        # ================== Train Generator ================== #\n",
    "        # ===================================================== #\n",
    "        opt_G.zero_grad()\n",
    "\n",
    "        D_fake = self.discriminator(fake_images)\n",
    "        # loss_G = -F.binary_cross_entropy_with_logits(D_fake, torch.zeros_like(D_fake))  # NOTE: original GAN loss\n",
    "        loss_G = F.binary_cross_entropy_with_logits(D_fake, torch.ones_like(D_fake))  # NOTE: non-saturating GAN loss\n",
    "\n",
    "        self.manual_backward(loss_G)\n",
    "        opt_G.step()\n",
    "\n",
    "        # ================================================= #\n",
    "        # ================== Log Results ================== #\n",
    "        # ================================================= #\n",
    "        self.log(\"loss_D\", loss_D, on_step=True, on_epoch=False, prog_bar=True)\n",
    "        self.log(\"loss_G\", loss_G, on_step=True, on_epoch=False, prog_bar=True)\n",
    "\n",
    "        return {\"loss_D\": loss_D, \"loss_G\": loss_G}\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt_D = optim.AdamW(self.discriminator.parameters(), **self.opt_d_params)\n",
    "        opt_G = optim.AdamW(self.generator.parameters(), **self.opt_g_params)\n",
    "        return [opt_D, opt_G], []\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        \"\"\"\n",
    "        Generate example images at the end of each epoch\n",
    "        \"\"\"\n",
    "        os.makedirs(\"output/vanilla_cifar10\", exist_ok=True)\n",
    "        file_path = f\"output/vanilla_cifar10/generator_epoch_{self.current_epoch}.png\"\n",
    "        z = torch.randn(64, self.z_dim).to(self.device)\n",
    "        generated_images = self.generator(z)\n",
    "        save_image(generated_images, file_path, nrow=8, normalize=True)\n",
    "        return super().on_train_epoch_end()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "ZDIM = 128\n",
    "OPT_G_PARAMS = {\n",
    "    \"lr\": 1.5e-4,\n",
    "    \"betas\": (0.5, 0.999),\n",
    "}\n",
    "OPT_D_PARAMS = {\n",
    "    \"lr\": 1e-4,\n",
    "    \"betas\": (0.5, 0.999),\n",
    "}\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "data_module = Cifar10DataModule(batch_size=BATCH_SIZE)\n",
    "data_module.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1,2]\n",
      "\n",
      "  | Name          | Type          | Params\n",
      "------------------------------------------------\n",
      "0 | generator     | Generator     | 1.8 M \n",
      "1 | discriminator | Discriminator | 390 K \n",
      "------------------------------------------------\n",
      "2.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "2.2 M     Total params\n",
      "8.894     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 196/196 [00:07<00:00, 26.24it/s, v_num=75, loss_D=1.300, loss_G=0.896]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 196/196 [00:07<00:00, 25.37it/s, v_num=75, loss_D=1.300, loss_G=0.896]\n"
     ]
    }
   ],
   "source": [
    "model = GANModule(OPT_G_PARAMS, OPT_D_PARAMS, ZDIM)\n",
    "trainer = pl.Trainer(max_epochs=20, accelerator=\"gpu\", devices=[0])\n",
    "trainer.fit(model, datamodule=data_module)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
