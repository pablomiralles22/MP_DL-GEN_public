{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqNMyyNmShVD"
      },
      "source": [
        "# Example local diffusion with FLUX\n",
        "\n",
        "This notebook requires at least 12GB of GPU RAM. The it runs a local uncensored version of the [FLUX.1-schnell](https://huggingface.co/black-forest-labs/FLUX.1-schnell) model and allows for some parameters.\n",
        "\n",
        "This will also serve as an example of CLIP encodings added to a vision model at a purely practical level.\n",
        "\n",
        "First, some dependencies that will simplify our calls to the model:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VjYy0F2gZIPR",
        "outputId": "1dc890df-c734-409b-f94f-94e756b7bc8e"
      },
      "outputs": [],
      "source": [
        "%cd /content\n",
        "!git clone -b totoro3 https://github.com/camenduru/ComfyUI /content/TotoroUI\n",
        "%cd /content/TotoroUI\n",
        "\n",
        "!pip install -q torchsde einops diffusers accelerate xformers==0.0.28.post2\n",
        "!apt -y install -qq aria2\n",
        "\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/black-forest-labs/FLUX.1-schnell/resolve/main/flux1-schnell.safetensors -d /content/TotoroUI/models/unet -o flux1-schnell.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/ae.sft -d /content/TotoroUI/models/vae -o ae.sft\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/clip_l.safetensors -d /content/TotoroUI/models/clip -o clip_l.safetensors\n",
        "!aria2c --console-log-level=error -c -x 16 -s 16 -k 1M https://huggingface.co/camenduru/FLUX.1-dev/resolve/main/t5xxl_fp8_e4m3fn.safetensors -d /content/TotoroUI/models/clip -o t5xxl_fp8_e4m3fn.safetensors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhnt4kkDTQRl"
      },
      "source": [
        "We import all necessary libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYF_Y2wvSaxv"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import torch\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import nodes\n",
        "from nodes import NODE_CLASS_MAPPINGS\n",
        "from totoro_extras import nodes_custom_sampler\n",
        "from totoro import model_management\n",
        "from IPython.display import display\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VMOVpjMLTTjT"
      },
      "source": [
        "Finally we load all necessary components for our model to properly work."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NxqqpEq4SgNe"
      },
      "outputs": [],
      "source": [
        "DualCLIPLoader = NODE_CLASS_MAPPINGS[\"DualCLIPLoader\"]() # CLIP object to embed text\n",
        "UNETLoader = NODE_CLASS_MAPPINGS[\"UNETLoader\"]() # UNET, the architecture to load flux1-schnell\n",
        "RandomNoise = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"RandomNoise\"]() # Noise generator\n",
        "BasicGuider = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicGuider\"]() # Text conditioning method\n",
        "KSamplerSelect = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"KSamplerSelect\"]() # Choose the noise sampler\n",
        "BasicScheduler = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"BasicScheduler\"]() # Choose the noise scheduler\n",
        "SamplerCustomAdvanced = nodes_custom_sampler.NODE_CLASS_MAPPINGS[\"SamplerCustomAdvanced\"]()\n",
        "VAELoader = NODE_CLASS_MAPPINGS[\"VAELoader\"]() # A VAE decodes the refined latent representation\n",
        "VAEDecode = NODE_CLASS_MAPPINGS[\"VAEDecode\"]() # Decoding algorithm for the VAE\n",
        "EmptyLatentImage = NODE_CLASS_MAPPINGS[\"EmptyLatentImage\"]() # Generate a small image (latent representation)\n",
        "\n",
        "# Load the models, not just the objects.\n",
        "with torch.inference_mode():\n",
        "    clip = DualCLIPLoader.load_clip(\"t5xxl_fp8_e4m3fn.safetensors\", \"clip_l.safetensors\", \"flux\")[0]\n",
        "    unet = UNETLoader.load_unet(\"flux1-schnell.safetensors\", \"fp8_e4m3fn\")[0]\n",
        "    vae = VAELoader.load_vae(\"ae.sft\")[0]\n",
        "\n",
        "def closestNumber(n, m):\n",
        "    q = int(n / m)\n",
        "    n1 = m * q\n",
        "    if (n * m) > 0:\n",
        "        n2 = m * (q + 1)\n",
        "    else:\n",
        "        n2 = m * (q - 1)\n",
        "    if abs(n - n1) < abs(n - n2):\n",
        "        return n1\n",
        "    return n2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0caf4L9vT7yv"
      },
      "outputs": [],
      "source": [
        "# @title Image generator\n",
        "positive_prompt = \"Wizard using fireball in a room too small\" #@param {type:\"string\"}\n",
        "\n",
        "# These are the recommended settings, play around if you want.\n",
        "steps = 6 #@param {type:\"slider\", min:0, max:50, step:1}\n",
        "width = 512 #@param {type:\"slider\", min:0, max:2048, step:256}\n",
        "height = 512 #@param {type:\"slider\", min:0, max:2048, step:256}\n",
        "n_img = 3 #@param {type:\"slider\", min:1, max:8, step:1}\n",
        "seed = 45 #@param {type:\"number\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YkC931gWUwsq"
      },
      "source": [
        "Flux generation algorithm:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "M8r0yhy1jpIR"
      },
      "outputs": [],
      "source": [
        "for x in range(n_img):\n",
        "  with torch.inference_mode():\n",
        "      sampler_name = \"euler\"\n",
        "      scheduler = \"simple\"\n",
        "\n",
        "      if seed == 0:\n",
        "          seed = random.randint(0, 18446744073709551615)\n",
        "      else:\n",
        "          seed += 23\n",
        "\n",
        "      # 1. Generate CLIP embeddings from the positive prompt.\n",
        "      cond, pooled = clip.encode_from_tokens(clip.tokenize(positive_prompt), return_pooled=True)\n",
        "      cond = [[cond, {\"pooled_output\": pooled}]]\n",
        "\n",
        "      # 2. Generate random noise\n",
        "      noise = RandomNoise.get_noise(seed)[0]\n",
        "\n",
        "      # 3. Influence the unet (FLUX) with text (cond)\n",
        "      guider = BasicGuider.get_guider(unet, cond)[0]\n",
        "\n",
        "      # 4. Choose noise sampler\n",
        "      sampler = KSamplerSelect.get_sampler(sampler_name)[0]\n",
        "\n",
        "      # 5. Choose the sigmas for the number of steps given using the scheduler given\n",
        "      sigmas = BasicScheduler.get_sigmas(unet, scheduler, steps, 1.0)[0]\n",
        "\n",
        "      # 6. Portray an empty latent space (or latent image)\n",
        "      latent_image = EmptyLatentImage.generate(closestNumber(width, 16), closestNumber(height, 16))[0]\n",
        "\n",
        "      # 7. Use the noise, guided by the text, to iteratively sample N times using the variances sigmas.\n",
        "      # The result is sample: the refined latent representation\n",
        "      sample, sample_denoised = SamplerCustomAdvanced.sample(noise, guider, sampler, sigmas, latent_image)\n",
        "      model_management.soft_empty_cache()\n",
        "\n",
        "      # 8. Decode the refined latent representation using the chosen VAE\n",
        "      decoded = VAEDecode.decode(vae, sample)[0].detach()\n",
        "\n",
        "      # 9. Save the image!\n",
        "      img = Image.fromarray(np.array(decoded*255, dtype=np.uint8)[0])\n",
        "      img.save(f\"/content/flux_{x}.png\")\n",
        "      display(img)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
